{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":36363,"databundleVersionId":4050810,"sourceType":"competition"},{"sourceId":2768643,"sourceType":"datasetVersion","datasetId":1688447},{"sourceId":4407134,"sourceType":"datasetVersion","datasetId":2556595},{"sourceId":4454029,"sourceType":"datasetVersion","datasetId":2607864},{"sourceId":7429897,"sourceType":"datasetVersion","datasetId":4323656},{"sourceId":7910796,"sourceType":"datasetVersion","datasetId":4647323},{"sourceId":7911066,"sourceType":"datasetVersion","datasetId":4647528},{"sourceId":7921148,"sourceType":"datasetVersion","datasetId":4654903},{"sourceId":7921218,"sourceType":"datasetVersion","datasetId":4654958},{"sourceId":7921255,"sourceType":"datasetVersion","datasetId":4654987},{"sourceId":7986674,"sourceType":"datasetVersion","datasetId":4701323},{"sourceId":7991608,"sourceType":"datasetVersion","datasetId":4704757},{"sourceId":8060420,"sourceType":"datasetVersion","datasetId":4754537},{"sourceId":8076272,"sourceType":"datasetVersion","datasetId":4766227},{"sourceId":8082205,"sourceType":"datasetVersion","datasetId":4770495},{"sourceId":8088067,"sourceType":"datasetVersion","datasetId":4774614},{"sourceId":8108723,"sourceType":"datasetVersion","datasetId":4789705},{"sourceId":8113842,"sourceType":"datasetVersion","datasetId":4793408},{"sourceId":8122969,"sourceType":"datasetVersion","datasetId":4800014},{"sourceId":8132439,"sourceType":"datasetVersion","datasetId":4806853},{"sourceId":8142683,"sourceType":"datasetVersion","datasetId":4814487},{"sourceId":8151394,"sourceType":"datasetVersion","datasetId":4821007},{"sourceId":8163931,"sourceType":"datasetVersion","datasetId":4830525},{"sourceId":8174157,"sourceType":"datasetVersion","datasetId":4838264},{"sourceId":8181131,"sourceType":"datasetVersion","datasetId":4843587},{"sourceId":8196792,"sourceType":"datasetVersion","datasetId":4855254},{"sourceId":8286209,"sourceType":"datasetVersion","datasetId":4921532},{"sourceId":8292433,"sourceType":"datasetVersion","datasetId":4926210},{"sourceId":104036025,"sourceType":"kernelVersion"}],"dockerImageVersionId":30262,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install /kaggle/input/rsna-2022-whl/{pydicom-2.3.0-py3-none-any.whl,pylibjpeg-1.4.0-py3-none-any.whl,python_gdcm-3.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl}\n!pip install /kaggle/input/rsna-2022-whl/{torch-1.12.1-cp37-cp37m-manylinux1_x86_64.whl,torchvision-0.13.1-cp37-cp37m-manylinux1_x86_64.whl}","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:24:18.030216Z","iopub.execute_input":"2024-05-06T02:24:18.031236Z","iopub.status.idle":"2024-05-06T02:25:28.440527Z","shell.execute_reply.started":"2024-05-06T02:24:18.031134Z","shell.execute_reply":"2024-05-06T02:25:28.439247Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/rsna-2022-whl/pydicom-2.3.0-py3-none-any.whl\nProcessing /kaggle/input/rsna-2022-whl/pylibjpeg-1.4.0-py3-none-any.whl\nProcessing /kaggle/input/rsna-2022-whl/python_gdcm-3.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pylibjpeg==1.4.0) (1.21.6)\npydicom is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nInstalling collected packages: python-gdcm, pylibjpeg\nSuccessfully installed pylibjpeg-1.4.0 python-gdcm-3.0.15\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mProcessing /kaggle/input/rsna-2022-whl/torch-1.12.1-cp37-cp37m-manylinux1_x86_64.whl\nProcessing /kaggle/input/rsna-2022-whl/torchvision-0.13.1-cp37-cp37m-manylinux1_x86_64.whl\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.12.1) (4.3.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.13.1) (9.1.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision==0.13.1) (2.28.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==0.13.1) (1.21.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1) (2022.6.15.2)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1) (3.3)\nInstalling collected packages: torch, torchvision\n  Attempting uninstall: torch\n    Found existing installation: torch 1.11.0\n    Uninstalling torch-1.11.0:\n      Successfully uninstalled torch-1.11.0\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.12.0\n    Uninstalling torchvision-0.12.0:\n      Successfully uninstalled torchvision-0.12.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nallennlp 2.10.0 requires protobuf==3.20.0, but you have protobuf 3.19.4 which is incompatible.\nallennlp 2.10.0 requires torch<1.12.0,>=1.10.0, but you have torch 1.12.1 which is incompatible.\nallennlp 2.10.0 requires torchvision<0.13.0,>=0.8.1, but you have torchvision 0.13.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-1.12.1 torchvision-0.13.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install /kaggle/input/rsna-weights/timm-0.5.4-py3-none-any.whl\n!pip install /kaggle/input/rsna-weights/tifffile-2022.8.8-py3-none-any.whl\n!pip install /kaggle/input/rsna-weights/einops-0.5.0-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:25:28.442921Z","iopub.execute_input":"2024-05-06T02:25:28.443293Z","iopub.status.idle":"2024-05-06T02:25:54.318176Z","shell.execute_reply.started":"2024-05-06T02:25:28.443256Z","shell.execute_reply":"2024-05-06T02:25:54.317051Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/rsna-weights/timm-0.5.4-py3-none-any.whl\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm==0.5.4) (0.13.1)\nRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from timm==0.5.4) (1.12.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm==0.5.4) (4.3.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision->timm==0.5.4) (2.28.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->timm==0.5.4) (1.21.6)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm==0.5.4) (9.1.1)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm==0.5.4) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm==0.5.4) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm==0.5.4) (2022.6.15.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm==0.5.4) (3.3)\nInstalling collected packages: timm\nSuccessfully installed timm-0.5.4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mProcessing /kaggle/input/rsna-weights/tifffile-2022.8.8-py3-none-any.whl\nRequirement already satisfied: numpy>=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tifffile==2022.8.8) (1.21.6)\n\u001b[31mERROR: Package 'tifffile' requires a different Python: 3.7.12 not in '>=3.8'\u001b[0m\u001b[31m\n\u001b[0mProcessing /kaggle/input/rsna-weights/einops-0.5.0-py3-none-any.whl\nInstalling collected packages: einops\nSuccessfully installed einops-0.5.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nos.listdir('/kaggle/input/mmdetection-2-17-offline')\n\n!pip install /kaggle/input/mmdetection-2-17-offline/mmcv_full-1.3.14-cp37-cp37m-linux_x86_64.whl --no-deps\n!pip install /kaggle/input/mmdetection-2-17-offline/pycocotools-2.0.2-cp37-cp37m-linux_x86_64.whl --no-deps\n!pip install /kaggle/input/mmdetection-2-17-offline/terminaltables-3.1.0-py3-none-any.whl --no-deps\n!pip install /kaggle/input/mmdetection-2-17-offline/pytest_runner-5.3.1-py3-none-any.whl --no-deps\n!pip install /kaggle/input/mmdetection-2-17-offline/mmpycocotools-12.0.3-cp37-cp37m-linux_x86_64.whl --no-deps\n!pip install /kaggle/input/mmdetection-2-17-offline/terminal-0.4.0-py3-none-any.whl --no-deps\n!pip install /kaggle/input/mmdetection-2-17-offline/mmdet-2.17.0-py3-none-any.whl --no-deps\n!pip install /kaggle/input/mmdetection-2-17-offline/addict-2.4.0-py3-none-any.whl --no-deps\n!pip install /kaggle/input/mmdetection-2-17-offline/yapf-0.31.0-py2.py3-none-any.whl --no-deps","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:25:54.320029Z","iopub.execute_input":"2024-05-06T02:25:54.320515Z","iopub.status.idle":"2024-05-06T02:26:17.272901Z","shell.execute_reply.started":"2024-05-06T02:25:54.320470Z","shell.execute_reply":"2024-05-06T02:26:17.271589Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/mmdetection-2-17-offline/mmcv_full-1.3.14-cp37-cp37m-linux_x86_64.whl\nInstalling collected packages: mmcv-full\nSuccessfully installed mmcv-full-1.3.14\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mProcessing /kaggle/input/mmdetection-2-17-offline/pycocotools-2.0.2-cp37-cp37m-linux_x86_64.whl\nInstalling collected packages: pycocotools\nSuccessfully installed pycocotools-2.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mProcessing /kaggle/input/mmdetection-2-17-offline/terminaltables-3.1.0-py3-none-any.whl\nInstalling collected packages: terminaltables\nSuccessfully installed terminaltables-3.1.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mProcessing /kaggle/input/mmdetection-2-17-offline/pytest_runner-5.3.1-py3-none-any.whl\nInstalling collected packages: pytest-runner\nSuccessfully installed pytest-runner-5.3.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mProcessing /kaggle/input/mmdetection-2-17-offline/mmpycocotools-12.0.3-cp37-cp37m-linux_x86_64.whl\nInstalling collected packages: mmpycocotools\nSuccessfully installed mmpycocotools-12.0.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mProcessing /kaggle/input/mmdetection-2-17-offline/terminal-0.4.0-py3-none-any.whl\nInstalling collected packages: terminal\nSuccessfully installed terminal-0.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mProcessing /kaggle/input/mmdetection-2-17-offline/mmdet-2.17.0-py3-none-any.whl\nInstalling collected packages: mmdet\nSuccessfully installed mmdet-2.17.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mProcessing /kaggle/input/mmdetection-2-17-offline/addict-2.4.0-py3-none-any.whl\nInstalling collected packages: addict\nSuccessfully installed addict-2.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mProcessing /kaggle/input/mmdetection-2-17-offline/yapf-0.31.0-py2.py3-none-any.whl\nInstalling collected packages: yapf\n  Attempting uninstall: yapf\n    Found existing installation: yapf 0.32.0\n    Uninstalling yapf-0.32.0:\n      Successfully uninstalled yapf-0.32.0\nSuccessfully installed yapf-0.31.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/rsnazoopublic')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:26:17.274427Z","iopub.execute_input":"2024-05-06T02:26:17.274795Z","iopub.status.idle":"2024-05-06T02:26:17.279965Z","shell.execute_reply.started":"2024-05-06T02:26:17.274759Z","shell.execute_reply":"2024-05-06T02:26:17.278810Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import os\nimport random\nimport re\nfrom dataclasses import dataclass\nfrom typing import Dict\nfrom typing import List\n\nimport albumentations\nimport cv2\nimport numpy as np\nimport pydicom\nimport tifffile\nimport torch\nimport torch.hub\nfrom albumentations import ReplayCompose\nfrom skimage import measure\nfrom torch.functional import Tensor\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport time","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:26:17.282799Z","iopub.execute_input":"2024-05-06T02:26:17.283157Z","iopub.status.idle":"2024-05-06T02:26:19.985357Z","shell.execute_reply.started":"2024-05-06T02:26:17.283124Z","shell.execute_reply":"2024-05-06T02:26:19.984495Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"!pip install einops\n!pip install monai","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:26:19.986670Z","iopub.execute_input":"2024-05-06T02:26:19.987329Z","iopub.status.idle":"2024-05-06T02:26:45.059818Z","shell.execute_reply.started":"2024-05-06T02:26:19.987285Z","shell.execute_reply":"2024-05-06T02:26:45.058415Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: einops in /opt/conda/lib/python3.7/site-packages (0.5.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting monai\n  Downloading monai-1.1.0-202212191849-py3-none-any.whl (1.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: torch>=1.8 in /opt/conda/lib/python3.7/site-packages (from monai) (1.12.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from monai) (1.21.6)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.8->monai) (4.3.0)\nInstalling collected packages: monai\nSuccessfully installed monai-1.1.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"cd /kaggle/input/cotr11112/CoTr_package","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:26:45.061643Z","iopub.execute_input":"2024-05-06T02:26:45.062009Z","iopub.status.idle":"2024-05-06T02:26:45.076560Z","shell.execute_reply.started":"2024-05-06T02:26:45.061974Z","shell.execute_reply":"2024-05-06T02:26:45.075586Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/cotr11112/CoTr_package\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from CoTr.network_architecture.DeTrans.DeformableTrans import DeformableTransformer","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:26:45.078084Z","iopub.execute_input":"2024-05-06T02:26:45.078612Z","iopub.status.idle":"2024-05-06T02:26:45.202677Z","shell.execute_reply.started":"2024-05-06T02:26:45.078569Z","shell.execute_reply":"2024-05-06T02:26:45.201761Z"},"trusted":true},"outputs":[{"name":"stdout","text":"This is CoTr\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pip install batchgenerators","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:26:45.203906Z","iopub.execute_input":"2024-05-06T02:26:45.204218Z","iopub.status.idle":"2024-05-06T02:26:59.722338Z","shell.execute_reply.started":"2024-05-06T02:26:45.204188Z","shell.execute_reply":"2024-05-06T02:26:59.721051Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting batchgenerators\n  Downloading batchgenerators-0.25.tar.gz (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.7/site-packages (from batchgenerators) (9.1.1)\nRequirement already satisfied: numpy>=1.10.2 in /opt/conda/lib/python3.7/site-packages (from batchgenerators) (1.21.6)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from batchgenerators) (1.7.3)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from batchgenerators) (0.19.3)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from batchgenerators) (1.0.2)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from batchgenerators) (0.18.2)\nCollecting unittest2\n  Downloading unittest2-1.1.0-py2.py3-none-any.whl (96 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: threadpoolctl in /opt/conda/lib/python3.7/site-packages (from batchgenerators) (3.1.0)\nRequirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->batchgenerators) (2021.11.2)\nRequirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image->batchgenerators) (2.5)\nRequirement already satisfied: imageio>=2.4.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->batchgenerators) (2.19.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->batchgenerators) (21.3)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->batchgenerators) (1.3.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->batchgenerators) (1.0.1)\nCollecting traceback2\n  Downloading traceback2-1.4.0-py2.py3-none-any.whl (16 kB)\nCollecting argparse\n  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\nRequirement already satisfied: six>=1.4 in /opt/conda/lib/python3.7/site-packages (from unittest2->batchgenerators) (1.15.0)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.2->scikit-image->batchgenerators) (5.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->scikit-image->batchgenerators) (3.0.9)\nCollecting linecache2\n  Downloading linecache2-1.0.0-py2.py3-none-any.whl (12 kB)\nBuilding wheels for collected packages: batchgenerators\n  Building wheel for batchgenerators (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for batchgenerators: filename=batchgenerators-0.25-py3-none-any.whl size=89025 sha256=48610a78f56b5ade1e6816472dac4a88f294f73d5fd72f0d8bb7d30bd50e9475\n  Stored in directory: /root/.cache/pip/wheels/99/4d/42/fc3d88bd520210fbd2ebdc5fa5383ecf58c9440cd92fc8660c\nSuccessfully built batchgenerators\nInstalling collected packages: linecache2, argparse, traceback2, unittest2, batchgenerators\nSuccessfully installed argparse-1.4.0 batchgenerators-0.25 linecache2-1.0.0 traceback2-1.4.0 unittest2-1.1.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ------------------------------------------------------------------------\n# CNN encoder\n# ------------------------------------------------------------------------\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport math\nfrom functools import partial\n\n\nclass Conv3d_wd(nn.Conv3d):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=(1,1,1), padding=(0,0,0), dilation=(1,1,1), groups=1, bias=False):\n        super(Conv3d_wd, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n\n    def forward(self, x):\n        weight = self.weight\n        weight_mean = weight.mean(dim=1, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True).mean(dim=4, keepdim=True)\n        weight = weight - weight_mean\n        # std = weight.view(weight.size(0), -1).std(dim=1).view(-1, 1, 1, 1, 1) + 1e-5\n        std = torch.sqrt(torch.var(weight.view(weight.size(0), -1), dim=1) + 1e-12).view(-1, 1, 1, 1, 1)\n        weight = weight / std.expand_as(weight)\n        return F.conv3d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n\n\ndef conv3x3x3(in_planes, out_planes, kernel_size, stride=(1, 1, 1), padding=(0, 0, 0), dilation=(1, 1, 1), bias=False, weight_std=False):\n    \"3x3x3 convolution with padding\"\n    #print(\"weight_std value\",weight_std)\n    if weight_std:\n        return Conv3d_wd(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n    else:\n        return nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n\n\ndef Norm_layer(norm_cfg, inplanes):\n    #print(\"in norm later\",norm_cfg)\n    if norm_cfg == 'BN':\n        out = nn.BatchNorm3d(inplanes)\n    elif norm_cfg == 'SyncBN':\n        out = nn.SyncBatchNorm(inplanes)\n    elif norm_cfg == 'GN':\n        out = nn.GroupNorm(16, inplanes)\n    elif norm_cfg == 'IN':\n        out = nn.InstanceNorm3d(inplanes,affine=True)\n\n    return out\n\n\ndef Activation_layer(activation_cfg, inplace=True):\n    #print(activation_cfg)\n    if activation_cfg == 'ReLU':\n        out = nn.ReLU(inplace=inplace)\n    elif activation_cfg == 'LeakyReLU':\n        out = nn.LeakyReLU(negative_slope=1e-2, inplace=inplace)\n\n    return out\n\n\nclass ResBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, norm_cfg, activation_cfg, stride=(1, 1, 1), downsample=None, weight_std=False):\n        super().__init__()\n        self.conv1 = conv3x3x3(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False, weight_std=weight_std)\n        self.norm1 = Norm_layer(norm_cfg, planes)\n        self.nonlin = Activation_layer(activation_cfg, inplace=True)\n        self.downsample = downsample\n\n    def forward(self, x):\n        residual = x\n        #print(\"in resblock\",x.shape)\n        out = self.conv1(x)\n        #print(\"in resblock after conv\",x.shape)\n        out = self.norm1(out)\n        #print(\"in resblock after norm1\",out.shape)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n            #print(\"in resblock after downsample\",residual.shape)\n\n        out += residual\n        out = self.nonlin(out)\n        #print(\"in resblock after nonlinear\",out.shape)\n        #print(\".................................\")\n        return out\n\n\nclass Backbone(nn.Module):\n\n    arch_settings = {\n        9: (ResBlock, (3, 3, 2))\n    }\n\n\n    def __init__(self,\n                 depth,\n                 in_channels=3,\n                 norm_cfg='BN',\n                 activation_cfg='ReLU',\n                 weight_std=False):\n        super(Backbone, self).__init__()\n\n        if depth not in self.arch_settings:\n            raise KeyError('invalid depth {} for resnet'.format(depth))\n        self.depth = depth\n        block, layers = self.arch_settings[depth]\n        self.inplanes = 64\n        self.conv1 = conv3x3x3(in_channels, 64, kernel_size=7, stride=(1, 2, 2), padding=3, bias=False, weight_std=weight_std)\n        self.norm1 = Norm_layer(norm_cfg, 64)\n        self.nonlin = Activation_layer(activation_cfg, inplace=True)\n        self.layer1 = self._make_layer(block, 192, layers[0], stride=(2, 2, 2), norm_cfg=norm_cfg, activation_cfg=activation_cfg, weight_std=weight_std)\n        self.layer2= self._make_layer(block, 384, layers[1], stride=(2, 2, 2), norm_cfg=norm_cfg, activation_cfg=activation_cfg, weight_std=weight_std)\n        self.layer3 = self._make_layer(block, 384, layers[2], stride=(2, 2, 2), norm_cfg=norm_cfg, activation_cfg=activation_cfg, weight_std=weight_std)\n        self.layers = []\n\n        for m in self.modules():\n            if isinstance(m, (nn.Conv3d, Conv3d_wd)):\n                m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n            elif isinstance(m, (nn.BatchNorm3d, nn.GroupNorm, nn.InstanceNorm3d, nn.SyncBatchNorm)):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=(1, 1, 1), norm_cfg='BN', activation_cfg='ReLU', weight_std=False):\n        downsample = None\n        ##print(\"value of stride\",stride)\n        #print(\"value of self.inplanes\",self.inplanes)\n        #print(\"value of planes * block.expansion\",planes * block.expansion)\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv3x3x3(\n                    self.inplanes,\n                    planes * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False, weight_std=weight_std), Norm_layer(norm_cfg, planes * block.expansion))\n\n        layers = []\n        layers.append(block(self.inplanes, planes, norm_cfg, activation_cfg, stride=stride, downsample=downsample, weight_std=weight_std))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, norm_cfg, activation_cfg, weight_std=weight_std))\n\n        #print(\"nn sequential layer\",nn.Sequential(*layers))\n        return nn.Sequential(*layers)\n\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, (nn.Conv3d, Conv3d_wd)):\n                m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')\n            elif isinstance(m, (nn.BatchNorm3d, nn.GroupNorm, nn.InstanceNorm3d, nn.SyncBatchNorm)):\n                if m.weight is not None:\n                    nn.init.constant_(m.weight, 1)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        out = []\n        x = self.conv1(x)\n        #print(\"After conv1.....\",x.shape)\n        x = self.norm1(x)\n       # print(\"After norm1.....\",x.shape)\n        x = self.nonlin(x)\n        out.append(x)\n        #print(\"After nonlin.....\",x.shape)\n        x = self.layer1(x)\n        out.append(x)\n        #print(\"After layer1.....\",x.shape)\n        x = self.layer2(x)\n        out.append(x)\n        #print(\"After layer2.....\",x.shape)\n        x = self.layer3(x)\n        out.append(x)\n        #print(\"After layer3.....\",x.shape)\n\n\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:26:59.724405Z","iopub.execute_input":"2024-05-06T02:26:59.724758Z","iopub.status.idle":"2024-05-06T02:26:59.772579Z","shell.execute_reply.started":"2024-05-06T02:26:59.724725Z","shell.execute_reply":"2024-05-06T02:26:59.771505Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nPositional encodings for the transformer.\n\"\"\"\nimport math\nimport torch\nfrom torch import nn\nfrom typing import Optional\nfrom torch import Tensor\n\nclass PositionEmbeddingSine(nn.Module):\n    \"\"\"\n    This is a more standard version of the position embedding, very similar to the one\n    used by the Attention is all you need paper, generalized to work on images.\n    \"\"\"\n    def __init__(self, num_pos_feats=[64, 64, 64], temperature=10000, normalize=False, scale=None):\n        super().__init__()\n        self.num_pos_feats = num_pos_feats\n        self.temperature = temperature\n        self.normalize = normalize\n        if scale is not None and normalize is False:\n            raise ValueError(\"normalize should be True if scale is passed\")\n        if scale is None:\n            scale = 2 * math.pi\n        self.scale = scale\n\n    def forward(self, x):\n        bs, c, d, h, w = x.shape\n        #print(\"inside position embedding sine the x shape is:\",x.shape)\n        mask = torch.zeros(bs, d, h, w, dtype=torch.bool).cuda()\n        #print(\"mask shape is:\",mask.shape)     # Create a mask for the positional embeddings (all False initially)\n        #print(\"inside position embedding sine the mask is:\",mask)   arrays of false\n        \n        assert mask is not None\n        not_mask = ~mask   #true values\n        \n        # Compute cumulative sums along each dimension\n        d_embed = not_mask.cumsum(1, dtype=torch.float32)\n        #print(\"d_embed shape after cumsum is:\",d_embed.shape)\n        y_embed = not_mask.cumsum(2, dtype=torch.float32)\n        #print(\"y_embed shape after cumsum is:\",y_embed.shape)\n        x_embed = not_mask.cumsum(3, dtype=torch.float32)\n        #print(\"x_embed shape after cumsum is:\",x_embed.shape)\n        \n        print(\"inside position embedding self normalize:\",self.normalize) \n        #value of sle.normalize is true\n        if self.normalize:\n            # Normalize the cumulative sums and apply scaling\n            eps = 1e-6\n            d_embed = (d_embed - 0.5) / (d_embed[:, -1:, :, :] + eps) * self.scale\n            #print(\"after normalize d_embed shape is:\",d_embed.shape)\n            y_embed = (y_embed - 0.5) / (y_embed[:, :, -1:, :] + eps) * self.scale\n            #print(\"after normalize y_embed shape is:\",y_embed.shape)\n            x_embed = (x_embed - 0.5) / (x_embed[:, :, :, -1:] + eps) * self.scale\n            #print(\"after normalize x_embed shape is:\",x_embed.shape)\n\n        dim_tx = torch.arange(self.num_pos_feats[0], dtype=torch.float32, device=x.device)\n        #print(\" dim_tx shape is:\",dim_tx.shape)\n        dim_tx = self.temperature ** (3 * (dim_tx // 3) / self.num_pos_feats[0])\n        #print(\"dim_tx shape is:\",dim_tx.shape)\n\n        dim_ty = torch.arange(self.num_pos_feats[1], dtype=torch.float32, device=x.device)\n        #print(\" dim_ty shape is:\", dim_ty.shape)\n        dim_ty = self.temperature ** (3 * (dim_ty // 3) / self.num_pos_feats[1])\n        #print(\" dim_ty shape is:\", dim_ty.shape)\n\n        dim_td = torch.arange(self.num_pos_feats[2], dtype=torch.float32, device=x.device)\n        #print(\"dim_td shape is:\",dim_td.shape)\n        dim_td = self.temperature ** (3 * (dim_td // 3) / self.num_pos_feats[2])\n        #print(\"dim_td shape is:\",dim_td.shape)\n\n        pos_x = x_embed[:, :, :, :, None] / dim_tx\n        #print(\" pos_x shape is:\",pos_x.shape)\n        pos_y = y_embed[:, :, :, :, None] / dim_ty\n        #print(\" pos_y shape is:\",pos_y.shape)\n        pos_d = d_embed[:, :, :, :, None] / dim_td\n        #print(\" pos_d shape is:\",pos_d.shape)\n\n        pos_x = torch.stack((pos_x[:, :, :, :, 0::2].sin(), pos_x[:, :, :, :, 1::2].cos()), dim=5).flatten(4)\n        #print(\"after stack pos_x shape is:\",pos_x.shape)\n        pos_y = torch.stack((pos_y[:, :, :, :, 0::2].sin(), pos_y[:, :, :, :, 1::2].cos()), dim=5).flatten(4)\n        #print(\"after stack pos_y shape is:\",pos_y.shape)\n        pos_d = torch.stack((pos_d[:, :, :, :, 0::2].sin(), pos_d[:, :, :, :, 1::2].cos()), dim=5).flatten(4)\n        #print(\"after stack pos_d shape is:\",pos_d.shape)\n\n        pos = torch.cat((pos_d, pos_y, pos_x), dim=4).permute(0, 4, 1, 2, 3)\n        #print(\"pos shape is:\",pos.shape)\n\n        #print(\".........................\")\n        return pos\n\n\ndef build_position_encoding(mode, hidden_dim):\n    N_steps = hidden_dim // 3\n    #value of mode is v2 and hidden dimensio is 384\n    if (hidden_dim % 3) != 0:\n        N_steps = [N_steps, N_steps, N_steps + hidden_dim % 3]\n    else:\n        N_steps = [N_steps, N_steps, N_steps]\n    \n    if mode in ('v2', 'sine'):\n        position_embedding = PositionEmbeddingSine(num_pos_feats=N_steps, normalize=True)\n    else:\n        raise ValueError(f\"not supported {mode}\")\n\n    return position_embedding\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:26:59.774441Z","iopub.execute_input":"2024-05-06T02:26:59.774829Z","iopub.status.idle":"2024-05-06T02:26:59.802384Z","shell.execute_reply.started":"2024-05-06T02:26:59.774798Z","shell.execute_reply":"2024-05-06T02:26:59.801303Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------------------------------------\n# 3D Deformable Self-attention\n# ------------------------------------------------------------------------\n# Modified from Deformable DETR\n# Copyright (c) 2020 SenseTime. All Rights Reserved.\n# Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n# ------------------------------------------------------------------------\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\n\ndef ms_deform_attn_core_pytorch_3D(value, value_spatial_shapes, sampling_locations, attention_weights):\n    N_, S_, M_, D_ = value.shape\n#     print(\"value shape\",value.shape)\n#     print(\"N\",N_)\n#     print(\"S\",S_)\n#     print(\"M\",M_)\n#     print(\"D\",D_)\n    _, Lq_, M_, L_, P_, _ = sampling_locations.shape\n#     print(\"sampling_locations shape\",sampling_locations.shape)\n#     print(\"Lq\",Lq_)\n#     print(\"M\",M_)\n#     print(\"L\",L_)\n#     print(\"P\",P_)\n    value_list = value.split([T_ * H_ * W_ for T_, H_, W_ in value_spatial_shapes], dim=1)\n    #print(\" value_list\", len(value_list))\n    sampling_grids = 2 * sampling_locations - 1\n    #print(\"sampling_grids\",sampling_grids.shape)\n    # sampling_grids = 3 * sampling_locations - 1\n    sampling_value_list = []\n    for lid_, (T_, H_, W_) in enumerate(value_spatial_shapes):\n       # print(\"value_spatial_shapes\",value_spatial_shapes.shape)\n#         print(\"T\",T_.shape)\n#         print(\"H\",H_.shape)\n#         print(\"W\",W_.shape)\n        value_l_ = value_list[lid_].flatten(2).transpose(1, 2).reshape(N_*M_, D_, T_, H_, W_)\n        #print(\"value_l_\",value_l_.shape)\n        sampling_grid_l_ = sampling_grids[:, :, :, lid_].transpose(1, 2).flatten(0, 1)[:,None,:,:,:]\n        #print(\"sampling_grid_l_\",sampling_grid_l_.shape)\n        sampling_value_l_ = F.grid_sample(value_l_, sampling_grid_l_.to(dtype=value_l_.dtype), mode='bilinear', padding_mode='zeros', align_corners=False)[:,:,0]\n        #print(\"sampling_value_l_\",sampling_value_l_.shape)\n        sampling_value_list.append(sampling_value_l_)\n        #print(\"sampling_value_list after append\",len(sampling_value_list))\n        \n    attention_weights = attention_weights.transpose(1, 2).reshape(N_*M_, 1, Lq_, L_*P_)\n   # print(\"attention_weights\",attention_weights.shape)\n    output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights).sum(-1).view(N_, M_*D_, Lq_)\n#     print(\"output\",output.shape)\n#     print(\"output.transpose(1, 2).contiguous()\",output.transpose(1, 2).contiguous().shape)\n#     print('.......................')\n    return output.transpose(1, 2).contiguous()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:26:59.804075Z","iopub.execute_input":"2024-05-06T02:26:59.804452Z","iopub.status.idle":"2024-05-06T02:26:59.818924Z","shell.execute_reply.started":"2024-05-06T02:26:59.804416Z","shell.execute_reply":"2024-05-06T02:26:59.817988Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------------------------------------\n# 3D Deformable Self-attention\n# ------------------------------------------------------------------------\n# Modified from Deformable DETR\n# Copyright (c) 2020 SenseTime. All Rights Reserved.\n# Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n# ------------------------------------------------------------------------\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport warnings\nimport math\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.nn.init import xavier_uniform_, constant_\n#from CoTr.network_architecture.DeTrans.ops.functions.ms_deform_attn_func import ms_deform_attn_core_pytorch_3D\n\nclass MSDeformAttn(nn.Module):\n    def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4):\n        \"\"\"\n        Multi-Scale Deformable Attention Module\n        :param d_model      hidden dimension\n        :param n_levels     number of feature levels\n        :param n_heads      number of attention heads\n        :param n_points     number of sampling points per attention head per feature level\n        \"\"\"\n        super().__init__()\n        if d_model % n_heads != 0:\n            raise ValueError('d_model must be divisible by n_heads, but got {} and {}'.format(d_model, n_heads))\n        _d_per_head = d_model // n_heads\n\n        self.im2col_step = 64\n\n        self.d_model = d_model\n        self.n_levels = n_levels\n        self.n_heads = n_heads\n        self.n_points = n_points\n\n        self.sampling_offsets = nn.Linear(d_model, n_heads * n_levels * n_points * 3)\n        self.attention_weights = nn.Linear(d_model, n_heads * n_levels * n_points)\n        self.value_proj = nn.Linear(d_model, d_model)\n        self.output_proj = nn.Linear(d_model, d_model)\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        constant_(self.sampling_offsets.weight.data, 0.)\n        thetas = torch.arange(self.n_heads, dtype=torch.float32) * (2.0 * math.pi / self.n_heads)\n        grid_init = torch.stack([thetas.cos(), thetas.sin()*thetas.cos(), thetas.sin()*thetas.sin()], -1)\n        grid_init = (grid_init / grid_init.abs().max(-1, keepdim=True)[0]).view(self.n_heads, 1, 1, 3).repeat(1, self.n_levels, self.n_points, 1)\n        for i in range(self.n_points):\n            grid_init[:, :, i, :] *= i + 1\n        with torch.no_grad():\n            self.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n        constant_(self.attention_weights.weight.data, 0.)\n        constant_(self.attention_weights.bias.data, 0.)\n        xavier_uniform_(self.value_proj.weight.data)\n        constant_(self.value_proj.bias.data, 0.)\n        xavier_uniform_(self.output_proj.weight.data)\n        constant_(self.output_proj.bias.data, 0.)\n\n    def forward(self, query, reference_points, input_flatten, input_spatial_shapes, input_level_start_index, input_padding_mask=None):\n        \"\"\"\n        :param query                       (N, Length_{query}, C)\n        :param reference_points            (N, Length_{query}, n_levels, 3)\n        :param input_flatten               (N, \\sum_{l=0}^{L-1} D_l \\cdot H_l \\cdot W_l, C)\n        :param input_spatial_shapes        (n_levels, 3), [(D_0, H_0, W_0), (D_1, H_1, W_1), ..., (D_{L-1}, H_{L-1}, W_{L-1})]\n        :param input_level_start_index     (n_levels, ), [0, D_0*H_0*W_0, D_0*H_0*W_0+D_1*H_1*W_1, D_0*H_0*W_0+D_1*H_1*W_1+D_2*H_2*W_2, ..., D_0*H_0*W_0+D_1*H_1*W_1+...+D_{L-1}*H_{L-1}*W_{L-1}]\n        :param input_padding_mask          (N, \\sum_{l=0}^{L-1} D_l \\cdot H_l \\cdot W_l), True for padding elements, False for non-padding elements\n\n        :return output                     (N, Length_{query}, C)\n        \"\"\"\n        N, Len_q, _ = query.shape\n        #print(\"query\",query.shape)\n        N, Len_q, _ = query.shape\n        #print(\"query..............\",query)\n        #print(\"query shape\",query.shape)\n        #print(\"N\",N)\n        #print(\"len_q\",Len_q)\n        N, Len_in, _ = input_flatten.shape\n        #print(\"N\",N)\n        #print(\"len_in\",Len_in)\n        assert (input_spatial_shapes[:, 0] * input_spatial_shapes[:, 1] * input_spatial_shapes[:, 2]).sum() == Len_in\n        \n        #print(\"input_flatten\",input_flatten.shape)\n        value = self.value_proj(input_flatten)\n        #print(\"value shape after linear\",value.shape)\n        #print(\"input_padding mask value\",input_padding_mask)\n        if input_padding_mask is not None:\n            value = value.masked_fill(input_padding_mask[..., None], float(0))\n        value = value.view(N, Len_in, self.n_heads, self.d_model // self.n_heads)\n       # print(\"value shape after view\",value.shape)\n        sampling_offsets = self.sampling_offsets(query).view(N, Len_q, self.n_heads, self.n_levels, self.n_points, 3)\n       # print(\"valuesampling_offsets after linear\",sampling_offsets.shape)\n        attention_weights = self.attention_weights(query).view(N, Len_q, self.n_heads, self.n_levels * self.n_points)\n       # print(\"value attention_weights after linear\",attention_weights.shape)\n        attention_weights = F.softmax(attention_weights, -1).view(N, Len_q, self.n_heads, self.n_levels, self.n_points)\n       # print(\"value attention_weights after softmax\",attention_weights.shape)\n\n        #print(\"reference_points.shape[-1]\",reference_points.shape[-1])\n        if reference_points.shape[-1] == 3:\n            offset_normalizer = torch.stack([input_spatial_shapes[..., 0], input_spatial_shapes[..., 2], input_spatial_shapes[..., 1]], -1)\n            #print(\"offset_normalizer\",offset_normalizer.shape)\n            #print(\"....\",input_spatial_shapes.shape)\n            sampling_locations = reference_points[:, :, None, :, None, :] \\\n                                 + sampling_offsets / offset_normalizer[None, None, None, :, None, :]\n           # print(\"sampling_locations\",sampling_locations.shape)\n\n        output = ms_deform_attn_core_pytorch_3D(value, input_spatial_shapes, sampling_locations, attention_weights)\n       # print(\"output\",output.shape)\n        output = self.output_proj(output)\n       # print(\"Final output\",output.shape)\n       # print(\">................................\")\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:26:59.820348Z","iopub.execute_input":"2024-05-06T02:26:59.821192Z","iopub.status.idle":"2024-05-06T02:26:59.850604Z","shell.execute_reply.started":"2024-05-06T02:26:59.821149Z","shell.execute_reply":"2024-05-06T02:26:59.849593Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------------------------------------\n# 3D Deformable Transformer\n# ------------------------------------------------------------------------\n# Modified from Deformable DETR \n# Copyright (c) 2020 SenseTime. All Rights Reserved.\n# Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n\nimport copy\nfrom typing import Optional, List\nimport math\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.nn.init import xavier_uniform_, constant_, normal_\n#from  CoTr.network_architecture.DeTrans.ops.modules import MSDeformAttn\nfrom  CoTr.network_architecture.DeTrans.position_encoding import build_position_encoding\n\nclass DeformableTransformer(nn.Module):\n    def __init__(self, d_model=256, nhead=8,\n                 num_encoder_layers=6, dim_feedforward=1024, dropout=0.1,\n                 activation=\"relu\", num_feature_levels=4, enc_n_points=4):\n        super().__init__()\n\n        self.d_model = d_model\n        self.nhead = nhead\n\n        encoder_layer = DeformableTransformerEncoderLayer(d_model, dim_feedforward,dropout, activation, num_feature_levels, nhead, enc_n_points)\n        self.encoder = DeformableTransformerEncoder(encoder_layer, num_encoder_layers)\n\n        self.level_embed = nn.Parameter(torch.Tensor(num_feature_levels, d_model))\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n        for m in self.modules():\n            if isinstance(m, MSDeformAttn):\n                m._reset_parameters()\n        normal_(self.level_embed)\n\n    def get_valid_ratio(self, mask):\n        _, D, H, W = mask.shape\n        #print(\"D\",D)\n        #print(\"H\",H)\n        #print(\"W\",W)\n        valid_D = torch.sum(~mask[:, :, 0, 0], 1)\n        #print(\"valid_D\",valid_D.shape)\n        valid_H = torch.sum(~mask[:, 0, :, 0], 1)\n        #print(\"valid_H\",valid_H.shape)\n        valid_W = torch.sum(~mask[:, 0, 0, :], 1)\n        #print(\"valid_W\",valid_W.shape)\n\n        valid_ratio_d = valid_D.float() / D\n        #print(\"valid_ratio_d\",valid_ratio_d.shape)\n        valid_ratio_h = valid_H.float() / H\n        #print(\"valid_ratio_h\",valid_ratio_h.shape)\n        valid_ratio_w = valid_W.float() / W\n        #print(\"valid_ratio_w\",valid_ratio_w.shape)\n        valid_ratio = torch.stack([valid_ratio_d, valid_ratio_w, valid_ratio_h], -1)\n        #print(\"valid_ratio\",valid_ratio.shape)\n        #print(\".............................\")\n        return valid_ratio\n\n    def forward(self, srcs, masks, pos_embeds):\n\n        # prepare input for encoder\n        src_flatten = []\n        mask_flatten = []\n        lvl_pos_embed_flatten = []\n        spatial_shapes = []\n       \n        for lvl, (src, mask, pos_embed) in enumerate(zip(srcs, masks, pos_embeds)):\n            #print(\"lvl\",lvl)\n            #print(\"src\",src.shape)\n            #print(\"mask\",mask.shape)\n            #print(\"pos_embed\",pos_embed.shape)\n            bs, c, d, h, w = src.shape\n            spatial_shape = (d, h, w)\n            #print(\"spatial_shape\",len(spatial_shape))\n            spatial_shapes.append(spatial_shape)\n            #print(\"spatial_shapes\",len(spatial_shapes))\n            src = src.flatten(2).transpose(1, 2)\n            #print(\"src after flatten and tranpose\",src.shape)\n            mask = mask.flatten(1)\n            #print(\"mask after flatten\",mask.shape)\n            pos_embed = pos_embed.flatten(2).transpose(1, 2)\n            #print(\"pos_embed after flatten and transpose\",pos_embed.shape)\n            \n            lvl_pos_embed = pos_embed + self.level_embed[lvl].view(1, 1, -1)\n            #print(\"lvl_pos_embed\",len(lvl_pos_embed))\n            lvl_pos_embed_flatten.append(lvl_pos_embed)\n            #print(\"lvl_pos_embed_flatten\",len(lvl_pos_embed))\n            src_flatten.append(src)\n            #print(\"src_flatten \",len(src_flatten))\n            mask_flatten.append(mask)\n            #print(\"mask_flatten\",len(mask_flatten))\n            \n        src_flatten = torch.cat(src_flatten, 1)\n        #print(\"src_flatten after concat \",src_flatten.shape)\n        mask_flatten = torch.cat(mask_flatten, 1)\n        #print(\"mask_flatten after concat\",mask_flatten.shape)\n        lvl_pos_embed_flatten = torch.cat(lvl_pos_embed_flatten, 1)\n        #print(\"lvl_pos_embed_flatten after concat\",lvl_pos_embed_flatten.shape)\n        \n        spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=src_flatten.device)\n        #print(\"spatial_shapes....\",spatial_shapes.shape)\n        level_start_index = torch.cat((spatial_shapes.new_zeros((1, )), spatial_shapes.prod(1).cumsum(0)[:-1]))\n        #print(\"level_start_index\",level_start_index.shape)\n        valid_ratios = torch.stack([self.get_valid_ratio(m) for m in masks], 1)\n        #print(\"valid_ratios\",valid_ratios.shape)\n\n        # encoder\n        memory = self.encoder(src_flatten, spatial_shapes, level_start_index, valid_ratios, lvl_pos_embed_flatten, mask_flatten)\n        #print(\"memory\",memory.shape)\n         \n        #print(\"............................\")    \n        return memory","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:26:59.855144Z","iopub.execute_input":"2024-05-06T02:26:59.855595Z","iopub.status.idle":"2024-05-06T02:26:59.880522Z","shell.execute_reply.started":"2024-05-06T02:26:59.855541Z","shell.execute_reply":"2024-05-06T02:26:59.879452Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class DeformableTransformerEncoderLayer(nn.Module):\n    def __init__(self,\n                 d_model=256, d_ffn=1024,\n                 dropout=0.1, activation=\"relu\",\n                 n_levels=4, n_heads=8, n_points=4):\n        super().__init__()\n\n        # self attention\n        self.self_attn = MSDeformAttn(d_model, n_levels, n_heads, n_points)\n        self.dropout1 = nn.Dropout(dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n\n        # ffn\n        self.linear1 = nn.Linear(d_model, d_ffn)\n        self.activation = _get_activation_fn(activation)\n        self.dropout2 = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(d_ffn, d_model)\n        self.dropout3 = nn.Dropout(dropout)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    @staticmethod\n    def with_pos_embed(tensor, pos):\n        return tensor if pos is None else tensor + pos\n\n    def forward_ffn(self, src):\n        src2 = self.linear2(self.dropout2(self.activation(self.linear1(src))))\n        #print(\"src2 after linear2\",src2.shape)\n        src = src + self.dropout3(src2)\n        #print(\"src after dropout 3\",src.shape)\n        src = self.norm2(src)\n        #print(\"src after norm2\",src.shape)\n       # print(\".............\")\n        return src\n\n    def forward(self, src, pos, reference_points, spatial_shapes, level_start_index, padding_mask=None):\n        # self attention\n        src2 = self.self_attn(self.with_pos_embed(src, pos), reference_points, src, spatial_shapes, level_start_index, padding_mask)\n       # print(\"src2 after self attn\",src2.shape)\n        src = src + self.dropout1(src2)\n       # print(\"src after dropout1\",src.shape)\n        src = self.norm1(src)\n        #print(\"src after norm 1\",src.shape)\n\n        # ffn\n        src = self.forward_ffn(src)\n       # print(\"src after forward ffn\",src.shape)\n        #print(\"...............\")\n        return src\n\nclass DeformableTransformerEncoder(nn.Module):\n    def __init__(self, encoder_layer, num_layers):\n        super().__init__()\n        self.layers = _get_clones(encoder_layer, num_layers)\n        self.num_layers = num_layers\n\n    @staticmethod\n    def get_reference_points(spatial_shapes, valid_ratios, device):\n        reference_points_list = []\n        for lvl, (D_, H_, W_) in enumerate(spatial_shapes):\n\n            ref_d, ref_y, ref_x = torch.meshgrid(torch.linspace(0.5, D_ - 0.5, D_, dtype=torch.float32, device=device),\n                                                 torch.linspace(0.5, H_ - 0.5, H_, dtype=torch.float32, device=device),\n                                                 torch.linspace(0.5, W_ - 0.5, W_, dtype=torch.float32, device=device))\n            \n            #print(\"ref_d\",ref_d.shape)\n            #print(\"ref_y\",ref_y.shape)\n            #print(\"ref_x\",ref_x.shape)\n            ref_d = ref_d.reshape(-1)[None] / (valid_ratios[:, None, lvl, 0] * D_)\n            #print(\"ref_d after reshape\",ref_d.shape)\n            ref_y = ref_y.reshape(-1)[None] / (valid_ratios[:, None, lvl, 2] * H_)\n            #print(\"ref_y after reshape\",ref_y.shape)\n            ref_x = ref_x.reshape(-1)[None] / (valid_ratios[:, None, lvl, 1] * W_)\n            #print(\"ref_x after reshape\",ref_x.shape)\n\n            ref = torch.stack((ref_d, ref_x, ref_y), -1)   # D W H\n            #print(\"ref\",ref.shape)\n            reference_points_list.append(ref)\n            \n        reference_points = torch.cat(reference_points_list, 1)\n        #print(\"reference_points\",reference_points.shape)\n        reference_points = reference_points[:, :, None] * valid_ratios[:, None]\n        #print(\"reference_points\",reference_points.shape)\n        #print(\".................\")\n        return reference_points\n\n    def forward(self, src, spatial_shapes, level_start_index, valid_ratios, pos=None, padding_mask=None):\n        output = src\n        reference_points = self.get_reference_points(spatial_shapes, valid_ratios, device=src.device)\n        #print(\"reference_points\",reference_points.shape)\n        for _, layer in enumerate(self.layers):\n            output = layer(output, pos, reference_points, spatial_shapes, level_start_index, padding_mask)\n            #print(\"output\",output.shape)\n        #print(\".......................\")\n        #print(\"output\",output.shape)\n        return output\n\n\ndef _get_clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n\n\ndef _get_activation_fn(activation):\n    \"\"\"Return an activation function given a string\"\"\"\n    #print(\"activation:\",activation)   value gelu\n    if activation == \"relu\":\n        return F.relu\n    if activation == \"gelu\":\n        return F.gelu\n    if activation == \"glu\":\n        return F.glu\n    raise RuntimeError(F\"activation should be relu/gelu, not {activation}.\")","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:26:59.881723Z","iopub.execute_input":"2024-05-06T02:26:59.882039Z","iopub.status.idle":"2024-05-06T02:26:59.909455Z","shell.execute_reply.started":"2024-05-06T02:26:59.882012Z","shell.execute_reply":"2024-05-06T02:26:59.908362Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n# from CoTr.network_architecture import CNNBackbone\nfrom CoTr.network_architecture.neural_network import SegmentationNetwork\n#from CoTr.network_architecture.DeTrans.DeformableTrans import DeformableTransformer\n#from CoTr.network_architecture.DeTrans.position_encoding import build_position_encoding\n\nclass Conv3d_wd(nn.Conv3d):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=(1,1,1), padding=(0,0,0), dilation=(1,1,1), groups=1, bias=False):\n        super(Conv3d_wd, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n\n    def forward(self, x):\n        weight = self.weight\n        #print(\"first........\",weight)\n        weight_mean = weight.mean(dim=1, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True).mean(dim=4, keepdim=True)\n       # print(\"sconeddd....\",weight_mean)\n        weight = weight - weight_mean\n       # print(\"third.....\",weight)\n        # std = weight.view(weight.size(0), -1).std(dim=1).view(-1, 1, 1, 1, 1) + 1e-5\n        std = torch.sqrt(torch.var(weight.view(weight.size(0), -1), dim=1) + 1e-12).view(-1, 1, 1, 1, 1)\n        weight = weight / std.expand_as(weight)\n        #print(weight)\n        return F.conv3d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n\ndef conv3x3x3(in_planes, out_planes, kernel_size, stride=(1, 1, 1), padding=(0, 0, 0), dilation=(1, 1, 1), groups=1, bias=False, weight_std=False):\n    \"3x3x3 convolution with padding\"\n    if weight_std:\n        return Conv3d_wd(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n    else:\n        return nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n\ndef Norm_layer(norm_cfg, inplanes):\n\n    if norm_cfg == 'BN':\n        out = nn.BatchNorm3d(inplanes)\n    elif norm_cfg == 'SyncBN':\n        out = nn.SyncBatchNorm(inplanes)\n    elif norm_cfg == 'GN':\n        out = nn.GroupNorm(16, inplanes)\n    elif norm_cfg == 'IN':\n        out = nn.InstanceNorm3d(inplanes,affine=True)\n\n    return out\n\n\ndef Activation_layer(activation_cfg, inplace=True):\n\n    if activation_cfg == 'ReLU':\n        out = nn.ReLU(inplace=inplace)\n    elif activation_cfg == 'LeakyReLU':\n        out = nn.LeakyReLU(negative_slope=1e-2, inplace=inplace)\n\n    return out\n\n\nclass Conv3dBlock(nn.Module):\n    def __init__(self,in_channels,out_channels,norm_cfg,activation_cfg,kernel_size,stride=(1, 1, 1),padding=(0, 0, 0),dilation=(1, 1, 1),bias=False,weight_std=False):\n        super(Conv3dBlock,self).__init__()\n        self.conv = conv3x3x3(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias, weight_std=weight_std)\n        self.norm = Norm_layer(norm_cfg, out_channels)\n        self.nonlin = Activation_layer(activation_cfg, inplace=True)\n    def forward(self,x):\n        x = self.conv(x)\n        x = self.norm(x)\n        x = self.nonlin(x)\n        return x\n\nclass ResBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, norm_cfg, activation_cfg, weight_std=False):\n        super().__init__()\n        self.resconv1 = Conv3dBlock(inplanes, planes, norm_cfg, activation_cfg, kernel_size=3, stride=1, padding=1, bias=False, weight_std=weight_std)\n        self.resconv2 = Conv3dBlock(planes, planes, norm_cfg, activation_cfg, kernel_size=3, stride=1, padding=1, bias=False, weight_std=weight_std)\n\n    def forward(self, x):\n        residual = x\n\n        out = self.resconv1(x)\n        out = self.resconv2(out)\n        out = out + residual\n\n        return out\n\nclass U_ResTran3D(nn.Module):\n    def __init__(self, norm_cfg='BN', activation_cfg='ReLU', img_size=None, num_classes=None, weight_std=False):\n        super(U_ResTran3D, self).__init__()\n\n        self.MODEL_NUM_CLASSES = num_classes\n\n        self.upsamplex2 = nn.Upsample(scale_factor=(1,2,2), mode='trilinear')\n\n        self.transposeconv_stage2 = nn.ConvTranspose3d(384, 384, kernel_size=(2,2,2), stride=(2,2,2), bias=False)\n        self.transposeconv_stage1 = nn.ConvTranspose3d(384, 192, kernel_size=(2,2,2), stride=(2,2,2), bias=False)\n        self.transposeconv_stage0 = nn.ConvTranspose3d(192, 64, kernel_size=(2,2,2), stride=(2,2,2), bias=False)\n\n        self.stage2_de = ResBlock(384, 384, norm_cfg, activation_cfg, weight_std=weight_std)\n        self.stage1_de = ResBlock(192, 192, norm_cfg, activation_cfg, weight_std=weight_std)\n        self.stage0_de = ResBlock(64, 64, norm_cfg, activation_cfg, weight_std=weight_std)\n\n        self.ds2_cls_conv = nn.Conv3d(384, self.MODEL_NUM_CLASSES, kernel_size=1)\n        self.ds1_cls_conv = nn.Conv3d(192, self.MODEL_NUM_CLASSES, kernel_size=1)\n        self.ds0_cls_conv = nn.Conv3d(64, self.MODEL_NUM_CLASSES, kernel_size=1)\n\n        self.cls_conv = nn.Conv3d(64, self.MODEL_NUM_CLASSES, kernel_size=1)\n\n        for m in self.modules():\n            if isinstance(m, (nn.Conv3d, Conv3d_wd, nn.ConvTranspose3d)):\n                m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')\n            elif isinstance(m, (nn.BatchNorm3d, nn.SyncBatchNorm, nn.InstanceNorm3d, nn.GroupNorm)):\n                if m.weight is not None:\n                    nn.init.constant_(m.weight, 1)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n        self.backbone = Backbone(depth=9, norm_cfg=norm_cfg, activation_cfg=activation_cfg, weight_std=weight_std)\n        total = sum([param.nelement() for param in self.backbone.parameters()])\n        print('  + Number of Backbone Params: %.2f(e6)' % (total / 1e6))\n\n        self.position_embed = build_position_encoding(mode='v2', hidden_dim=384)\n        self.encoder_Detrans = DeformableTransformer(d_model=384, dim_feedforward=1536, dropout=0.1, activation='gelu', num_feature_levels=2, nhead=6, num_encoder_layers=6, enc_n_points=4)\n        total = sum([param.nelement() for param in self.encoder_Detrans.parameters()])\n        print('  + Number of Transformer Params: %.2f(e6)' % (total / 1e6))\n\n    def posi_mask(self, x):\n\n        x_fea = []\n        x_posemb = []\n        masks = []\n        for lvl, fea in enumerate(x):\n            #print(\"in posimask value of lvl is:\",lvl)\n            if lvl > 1:\n                x_fea.append(fea)\n                #print(\"after appending fea....\",len(x_fea))\n                x_posemb.append(self.position_embed(fea))\n                masks.append(torch.zeros((fea.shape[0], fea.shape[2], fea.shape[3], fea.shape[4]), dtype=torch.bool).cuda())\n       # print(\"..........................\")\n        return x_fea, masks, x_posemb\n\n\n    def forward(self, inputs):\n        # # %%%%%%%%%%%%% CoTr\n        x_convs = self.backbone(inputs)\n        #print(\"after x_convs......\",len(x_convs))\n        x_fea, masks, x_posemb = self.posi_mask(x_convs)\n        #print(\"x_fea after posimask\",len(x_fea))\n        #print(\"masks after posimask\",len(masks))\n        #print(\"x_posemb after posimask\",len(x_posemb))\n        x_trans = self.encoder_Detrans(x_fea, masks, x_posemb)\n        #print(\"x_trans after encoder_Detrans\",x_trans.shape)\n       \n\n        # # Single_scale\n        # # x = self.transposeconv_stage2(x_trans.transpose(-1, -2).view(x_convs[-1].shape))\n        # # skip2 = x_convs[-2]\n        # Multi-scale   \n#         x = self.transposeconv_stage2(x_trans[:, x_fea[0].shape[-3]*x_fea[0].shape[-2]*x_fea[0].shape[-1]::].transpose(-1, -2).view(x_convs[-1].shape)) # x_trans length: 12*24*24+6*12*12=7776\n#         skip2 = x_trans[:, 0:x_fea[0].shape[-3]*x_fea[0].shape[-2]*x_fea[0].shape[-1]].transpose(-1, -2).view(x_convs[-2].shape)\n        \n#         x = x + skip2\n#         x = self.stage2_de(x)\n#         ds2 = self.ds2_cls_conv(x)\n\n#         x = self.transposeconv_stage1(x)\n#         skip1 = x_convs[-3]\n#         x = x + skip1\n#         x = self.stage1_de(x)\n#         ds1 = self.ds1_cls_conv(x)\n\n#         x = self.transposeconv_stage0(x)\n#         skip0 = x_convs[-4]\n#         x = x + skip0\n#         x = self.stage0_de(x)\n#         ds0 = self.ds0_cls_conv(x)\n\n\n#         result = self.upsamplex2(x)\n#         result = self.cls_conv(result)\n\n        return x_trans\n\n\nclass ResTranUnet(SegmentationNetwork):\n    \"\"\"\n    ResTran-3D Unet\n    \"\"\"\n    def __init__(self, norm_cfg='BN', activation_cfg='ReLU', img_size=None, num_classes=None, weight_std=False, deep_supervision=False):\n        super().__init__()\n        self.do_ds = False\n        self.U_ResTran3D = U_ResTran3D(norm_cfg, activation_cfg, img_size, num_classes, weight_std) # U_ResTran3D\n\n        if weight_std==False:\n            self.conv_op = nn.Conv3d\n        else:\n            self.conv_op = Conv3d_wd\n        if norm_cfg=='BN':\n            self.norm_op = nn.BatchNorm3d\n        if norm_cfg=='SyncBN':\n            self.norm_op = nn.SyncBatchNorm\n        if norm_cfg=='GN':\n            self.norm_op = nn.GroupNorm\n        if norm_cfg=='IN':\n            self.norm_op = nn.InstanceNorm3d\n        self.dropout_op = nn.Dropout3d\n        self.num_classes = num_classes\n        self._deep_supervision = deep_supervision\n        self.do_ds = deep_supervision\n\n    def forward(self, x):\n        seg_output = self.U_ResTran3D(x)\n        if self._deep_supervision and self.do_ds:\n            return seg_output\n        else:\n            return seg_output[0]\n          \nprint(\"hello\")","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:26:59.911177Z","iopub.execute_input":"2024-05-06T02:26:59.911599Z","iopub.status.idle":"2024-05-06T02:27:00.070425Z","shell.execute_reply.started":"2024-05-06T02:26:59.911557Z","shell.execute_reply":"2024-05-06T02:27:00.069340Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n\nPlease cite the following paper when using nnUNet:\n\nIsensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n\n\nIf you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n\nhello\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"class DeformableTransformerWithClassification(nn.Module):\n    def __init__(self, in_channels, img_size, patch_size, num_classes):\n        super(DeformableTransformerWithClassification, self).__init__()\n        self.features = U_ResTran3D(norm_cfg='BN', activation_cfg='ReLU', img_size=128, num_classes=3, weight_std=False)\n        self.avgpool = nn.AdaptiveAvgPool1d((1))  # Adaptive average pooling to get a fixed-size output\n        self.fc = nn.Linear(384, num_classes)  # Fully connected layer for classification\n\n    def forward(self, x):\n        x = self.features(x)  # Get the intermediate representations from the features module\n        #print(x.shape, \"after features\")  # Print the shape of the last intermediate representation\n        x = x.transpose(1, 2)\n        #print(\"after transpose\",x.shape)\n        x = self.avgpool(x)  # Apply average pooling to the last intermediate representation\n        #print(\"after average pooling\",x.shape)  # Print the shape after average pooling\n        x = torch.flatten(x, 1)  # Flatten the feature map\n        #print(\"after flatten\",x.shape)  # Print the shape after flattening\n        x = self.fc(x)  # Classification head\n        #print(\"after linear\",x.shape)\n        x = torch.sigmoid(x)  # Apply sigmoid activation for multi-label classification\n        #print(\"after sigmoid\",x.shape)\n        return x\n    \nmodel=DeformableTransformerWithClassification(in_channels=3,img_size=(40, 256, 256), patch_size=(8,8,8), num_classes=8)\n#print(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:27:00.073168Z","iopub.execute_input":"2024-05-06T02:27:00.073510Z","iopub.status.idle":"2024-05-06T02:27:00.690241Z","shell.execute_reply.started":"2024-05-06T02:27:00.073479Z","shell.execute_reply":"2024-05-06T02:27:00.689228Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:120: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n","output_type":"stream"},{"name":"stdout","text":"  + Number of Backbone Params: 20.54(e6)\n  + Number of Transformer Params: 9.32(e6)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel=DeformableTransformerWithClassification(in_channels=3,img_size=(40, 256, 256), patch_size=(8,8,8), num_classes=8).to(device)\n# model = U_ResTran3D(norm_cfg='BN', activation_cfg='ReLU', img_size=128, num_classes=3, weight_std=False).to(device)\ninput=torch.rand(1,3,40,256,256).to(device)\noutput=model(input)\n#print(output.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:27:00.691592Z","iopub.execute_input":"2024-05-06T02:27:00.692022Z","iopub.status.idle":"2024-05-06T02:27:04.932978Z","shell.execute_reply.started":"2024-05-06T02:27:00.691990Z","shell.execute_reply":"2024-05-06T02:27:04.932097Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:120: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n","output_type":"stream"},{"name":"stdout","text":"  + Number of Backbone Params: 20.54(e6)\n  + Number of Transformer Params: 9.32(e6)\n","output_type":"stream"},{"name":"stderr","text":"/kaggle/input/cotr11112/CoTr_package/CoTr/network_architecture/DeTrans/position_encoding.py:41: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n  dim_tx = self.temperature ** (3 * (dim_tx // 3) / self.num_pos_feats[0])\n/kaggle/input/cotr11112/CoTr_package/CoTr/network_architecture/DeTrans/position_encoding.py:44: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n  dim_ty = self.temperature ** (3 * (dim_ty // 3) / self.num_pos_feats[1])\n/kaggle/input/cotr11112/CoTr_package/CoTr/network_architecture/DeTrans/position_encoding.py:47: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n  dim_td = self.temperature ** (3 * (dim_td // 3) / self.num_pos_feats[2])\n/opt/conda/lib/python3.7/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2895.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"@dataclass\nclass BatchSlice:\n    i_from: int\n    i_to: int\n    i_start: int\n\n\ndef get_slices(batch: Tensor, dim=1, window: int = 16, overlap: int = 8) -> List[BatchSlice]:\n    num_imgs = batch.size(dim)\n    if num_imgs <= window:\n        return [BatchSlice(0, num_imgs, 0)]\n    stride = window - overlap\n    result = []\n    current_idx = 0\n    while True:\n        next_idx = current_idx + window\n\n        if next_idx >= num_imgs:\n            current_idx = num_imgs - window\n            offset = overlap // 2 if current_idx > 0 else 0\n            next_idx = num_imgs\n            result.append(BatchSlice(current_idx, next_idx, offset))\n            break\n        else:\n            offset = overlap // 2 if current_idx > 0 else 0\n            result.append(BatchSlice(current_idx, next_idx, offset))\n        current_idx += stride\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:27:04.934113Z","iopub.execute_input":"2024-05-06T02:27:04.934408Z","iopub.status.idle":"2024-05-06T02:27:04.945138Z","shell.execute_reply.started":"2024-05-06T02:27:04.934382Z","shell.execute_reply":"2024-05-06T02:27:04.944029Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import pandas as pd\ndef _read_labels():\n    labels_df = pd.read_csv(\"/kaggle/input/rsna-2022-cervical-spine-fracture-detection/train.csv\")\n    labels_dict = {}\n    for index, row in labels_df.iterrows():\n        cube_id = row['StudyInstanceUID']\n        overall_patient = row['patient_overall']\n        c1, c2, c3, c4, c5, c6, c7 = row['C1'], row['C2'], row['C3'], row['C4'], row['C5'], row['C6'], row['C7']\n        labels_dict[cube_id] = [overall_patient, c1, c2, c3, c4, c5, c6, c7]\n    return labels_dict\n\nlabels_dict = _read_labels()\n#print(labels_dict)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:27:04.946297Z","iopub.execute_input":"2024-05-06T02:27:04.946645Z","iopub.status.idle":"2024-05-06T02:27:05.178866Z","shell.execute_reply.started":"2024-05-06T02:27:04.946616Z","shell.execute_reply":"2024-05-06T02:27:05.177988Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def combine_scan(scan_dir: str, size=512, fix_monochrome: bool = True) -> np.ndarray:\n    num_files = len(os.listdir(scan_dir))\n    images = []\n    offset = 0\n    first = None\n    last = None\n    files = []\n    for i in range(num_files):\n        dpath = os.path.join(scan_dir, f\"{i + offset}.dcm\")\n        if i == 0:\n            while not os.path.exists(dpath):\n                offset += 1\n                dpath = os.path.join(scan_dir, f\"{i + offset}.dcm\")\n        files.append(dpath)\n\n    for dpath in files[::2]:\n        ds = pydicom.dcmread(dpath)\n        if not first:\n            first = ds\n        last = ds\n\n        data = ds.pixel_array\n        data = cv2.resize(data, (size, size))\n        if fix_monochrome and ds.PhotometricInterpretation == \"MONOCHROME1\":\n            data = np.amax(data) - data\n        images.append(data)\n\n    if first and last:\n        if last.ImagePositionPatient[2] > first.ImagePositionPatient[2]:\n            images = images[::-1]\n    return np.array(images)\n\nclass DatasetSeg(Dataset):\n    def __init__(\n            self,\n            dataset_dir: str,\n            cases: List[str],\n    ):\n        self.dataset_dir = dataset_dir\n        self.cases = cases\n\n    def __getitem__(self, i):\n        cube_id = self.cases[i]\n        image_cube = combine_scan(os.path.join(self.dataset_dir, cube_id), size=256)\n        image_mean = image_cube.mean()\n        image_std = image_cube.std()\n        h = image_cube.shape[0]\n\n        images = image_cube\n        if h % 32 > 0:\n            tmp = np.zeros(((h // 32 + 1) * 32, 256, 256))\n            tmp[:h] = images\n            images = tmp\n        images = (images - image_mean) / image_std\n        images = np.expand_dims(images, 0)\n        sample = {}\n        sample['image'] = torch.from_numpy(images).float()\n        sample['cube_id'] = cube_id\n        sample['h'] = h\n        return sample\n\n    def __len__(self):\n        return len(self.cases)\n\n\ncrop_augs =  albumentations.ReplayCompose([\n            albumentations.LongestMaxSize(256),\n            albumentations.PadIfNeeded(256, 256, border_mode=cv2.BORDER_CONSTANT),\n        ])\n\nclass DatasetCrops(Dataset):\n    def __init__(\n            self,\n            dataset_dir: str,\n            cases: List[str],\n            transforms=crop_augs,\n            slice_size=40,\n    ):\n        self.dataset_dir = dataset_dir\n        self.transforms = transforms\n        self.slice_size = slice_size\n        self.cases = cases\n\n    def __getitem__(self, i):\n        cube_id = self.cases[i]\n#         mask_cube = tifffile.imread(os.path.join(\"seg_preds\", f\"{cube_id}.tif\"))\n        mask_cube_path = os.path.join(\"/kaggle/input/masks-part1\", f\"{cube_id}.tif\")\n        if os.path.exists(mask_cube_path):\n            mask_cube = tifffile.imread(mask_cube_path)\n        else:\n            # If the file is not found in the first directory, try the second directory\n            mask_cube_path = os.path.join(\"/kaggle/input/masks-part2\", f\"{cube_id}.tif\")\n            if os.path.exists(mask_cube_path):\n                mask_cube = tifffile.imread(mask_cube_path)\n            else:\n                mask_cube_path = os.path.join(\"/kaggle/input/masks-part3\", f\"{cube_id}.tif\")\n                if os.path.exists(mask_cube_path):\n                    mask_cube = tifffile.imread(mask_cube_path)\n                else:\n                    mask_cube_path = os.path.join(\"/kaggle/input/masks-part4\", f\"{cube_id}.tif\")\n                    if os.path.exists(mask_cube_path):\n                        mask_cube = tifffile.imread(mask_cube_path)\n                    else:\n                        mask_cube_path = os.path.join(\"/kaggle/input/masks-part5\", f\"{cube_id}.tif\")\n                        if os.path.exists(mask_cube_path):\n                            mask_cube = tifffile.imread(mask_cube_path)\n                        else:\n                            mask_cube = torch.rand(256, 256, 256)\n                            mask_cube = mask_cube.cpu().numpy().astype(np.int)\n                            print(\"Error: mask_cube.tif not found in both directories.\")\n\n        image_cube = combine_scan(os.path.join(self.dataset_dir, cube_id) ,size=512)\n        boxes = {}\n        for rprop in measure.regionprops(mask_cube):\n            boxes[rprop.label] = rprop.bbox, rprop.area\n\n        image_mean = image_cube.mean()\n        image_std = image_cube.std()\n        slice_size = self.slice_size\n        all_images = []\n#         labels = np.zeros((8,))\n        labels = labels_dict[cube_id]\n        for li in range(1, 8):\n            if li not in boxes:\n                all_images.append(np.zeros((3, self.slice_size, 256, 256)))\n            else:\n                bbox, area = boxes[li]\n                z1, z2 = bbox[0], bbox[3]\n                y1, y2 = max(bbox[1] - 16, 0), min(bbox[4] + 16, 256)\n                x1, x2 = max(bbox[2] - 16, 0), min(bbox[5] + 16, 256)\n                # if z2 - z1 < slice_size:\n                #     z1 = random.randint(max(z2 - slice_size, 0), z1)\n                #     z2 = z1 + slice_size\n                # todo: verify\n                if z2 - z1 < slice_size:\n                    diff = (slice_size - z2 + z1) // 2\n                    z1 = max(0, z1 - diff)\n                    z2 = z1 + slice_size\n                images = image_cube[z1:z2, y1 * 2:y2 * 2, x1 * 2:x2 * 2].copy()\n                masks = mask_cube[z1:z2, y1:y2, x1:x2].copy()\n                slice_size = self.slice_size\n\n                replay = None\n                image_crops = []\n                mask_crops = []\n                for i in range(images.shape[0]):\n                    image = images[i]\n                    mask = masks[i]\n                    h, w, = mask.shape\n                    mask = cv2.resize(mask, (w * 2, h * 2), interpolation=cv2.INTER_NEAREST)\n                    if replay is None:\n                        sample = self.transforms(image=image, mask=mask)\n                        replay = sample[\"replay\"]\n                    else:\n                        sample = ReplayCompose.replay(replay, image=image, mask=mask)\n                    image_ = sample[\"image\"]\n                    image_crops.append(image_)\n                    mask_crops.append(sample[\"mask\"])\n                images = np.array(image_crops).astype(np.float32)\n                masks = np.array(mask_crops).astype(np.float32)\n                images = np.expand_dims(images, -1)\n                masks = np.expand_dims(masks, -1)\n                images = (images - image_mean) / image_std\n\n                images = np.concatenate([images, images, masks], axis=-1)\n                h = images.shape[0]\n                if h > slice_size:\n                    images = images[: slice_size]\n                    all_images.append(np.moveaxis(images, -1, 0))\n                    images = images[-slice_size:]\n                    all_images.append(np.moveaxis(images, -1, 0))\n                else:\n                    if h != slice_size:\n                        tmp = np.zeros((slice_size, *images.shape[1:]))\n                        tmp[:h] = images\n                        images = tmp\n                    all_images.append(np.moveaxis(images, -1, 0))\n\n        sample = {}\n        sample['image'] = torch.from_numpy(np.array(all_images)).float()\n        sample['label'] = torch.from_numpy(np.array(labels)).float()\n        sample['cube_id'] = cube_id\n        return sample\n\n    def __len__(self):\n        return len(self.cases)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:27:05.180332Z","iopub.execute_input":"2024-05-06T02:27:05.180637Z","iopub.status.idle":"2024-05-06T02:27:05.227059Z","shell.execute_reply.started":"2024-05-06T02:27:05.180610Z","shell.execute_reply":"2024-05-06T02:27:05.225849Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from torch.utils.data import random_split\nimport numpy as np\nfrom torch.utils.data import Subset \n\nvalidation_split = 0.1\ntest_split = 0.1\n\ndataset_dir = \"/kaggle/input/rsna-2022-cervical-spine-fracture-detection/train_images/\"\n\ncases = os.listdir(dataset_dir)\n# Create dataset\ndataset = DatasetCrops(dataset_dir=dataset_dir, cases=cases)\n\n# Compute sizes\ntrain_size = int(len(dataset) * (1 - validation_split - test_split))\nval_test_size = len(dataset) - train_size\nval_size = int(val_test_size / 2)\ntest_size = val_test_size - val_size\n\n# Random split into train, validation, and test datasets\ntrain_dataset, val_test_dataset = random_split(dataset, [train_size, val_test_size])\nval_dataset, test_dataset = random_split(val_test_dataset, [val_size, test_size])\n\n# train_dataset = Subset(train_dataset, range(5))\n# val_dataset = Subset(val_dataset, range(5))\n# test_dataset = Subset(test_dataset, range(5))\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\nprint(len(train_loader), len(val_loader), len(test_loader)) \n\nnum_train_images = len(train_dataset)\n\n# Calculate number of images in validation dataset\nnum_val_images = len(val_dataset)\n\n# Calculate number of images in test dataset\nnum_test_images = len(test_dataset)\n\n# Print the counts\nprint(f\"Number of images in training dataset: {num_train_images}\")\nprint(f\"Number of images in validation dataset: {num_val_images}\")\nprint(f\"Number of images in test dataset: {num_test_images}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:27:05.228594Z","iopub.execute_input":"2024-05-06T02:27:05.228942Z","iopub.status.idle":"2024-05-06T02:27:05.570468Z","shell.execute_reply.started":"2024-05-06T02:27:05.228904Z","shell.execute_reply":"2024-05-06T02:27:05.569472Z"},"trusted":true},"outputs":[{"name":"stdout","text":"1615 202 202\nNumber of images in training dataset: 1615\nNumber of images in validation dataset: 202\nNumber of images in test dataset: 202\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from torch import nn\nimport torch.optim as optim\nfrom sklearn.metrics import accuracy_score\nfrom torch.utils.data import random_split\n\nlearning_rate = 0.001  # Define the learning rate\n\ndef train_model(model, imgs):\n        preds = []\n        with torch.no_grad():\n            for i in range(len(imgs)):\n                with torch.cuda.amp.autocast():\n#                     output = model(imgs[i:i + 1])[\"cls\"][0]\n                    output = model(imgs[i:i + 1])\n                pred_slice = (output.float()).cpu().numpy().astype(np.float32)\n#                 with torch.cuda.amp.autocast():\n#                     output = model(torch.flip(imgs[i:i + 1], dims=(-1,)))\n                pred_slice += torch.sigmoid(output.float()).cpu().numpy().astype(np.float32)\n                pred_slice /= 2\n                preds.append(pred_slice)\n                break\n        preds = np.max(np.array(preds), axis=0)\n        preds[np.isnan(preds)] = 0.01\n        return preds\n\ndef train_classification(model, cases: List, num_epochs: int = 1):\n#     dataset = DatasetCrops(dataset_dir=dataset_dir, cases=cases)\n#     train_size = int(len(dataset) * (1 - validation_split))\n#     val_size = len(dataset) - train_size\n#     train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n#     train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n#     val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n    \n    \n    # Define loss criterion for multi-label classification\n    loss_criterion = nn.BCEWithLogitsLoss()  # You can adjust the loss function as needed\n    best_metric = -1\n    best_metric_epoch = -1\n    best_metrics_epochs_and_time = [[], [], []]\n    total_start=time.time()\n    for epoch in range(start_epoch, num_epochs):\n        print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n        \n        # Training phase\n        train_loss = 0.0\n        train_preds = []\n        train_labels = []\n        \n        for sample in tqdm(train_loader, desc=\"Training\"):\n            imgs = sample[\"image\"].cuda().float()[0]\n            cube_id = sample[\"cube_id\"][0]\n            with torch.no_grad():\n                preds = []\n#                 for model in models:\n                model.train()  # Set the model to training mode\n                x = train_model(model, imgs)\n                preds.append(x.squeeze())\n                preds = np.average(np.array(preds), axis=0)\n                preds = np.clip(preds, 0.01, 0.99)\n                \n                print(sample['label'].shape)\n                print(torch.tensor(preds).shape)\n                loss = loss_criterion(torch.tensor(preds).squeeze(), sample['label'].squeeze())  # Calculate loss\n                loss.requires_grad = True\n                print(loss)\n                # Backpropagation and optimization (assuming models are trainable)\n#                 for model in models:\n#                     model.train()  # Set the model to training mode\n                model.zero_grad()  # Zero the gradients\n                loss.backward()  # Backpropagate the gradients\n                optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Define optimizer\n                optimizer.step()  # Update model parameters\n                \n                # Compute metrics\n                train_loss += loss.item() * imgs.size(0)\n                train_preds.extend(preds)\n                train_labels.extend(sample['label'].squeeze().cpu().numpy())\n        \n#         print(train_labels.shape, train_preds.shape)\n        train_loss /= len(train_loader.dataset)\n        train_accuracy = accuracy_score(np.array(train_labels), np.array(train_preds) >= 0.5)\n        print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n        \n        # Validation phase\n        val_loss = 0.0\n        val_preds = []\n        val_labels = []\n        model.eval()  # Set the model to evaluation mode\n        with torch.no_grad():\n            for val_sample in tqdm(val_loader, desc=\"Validation\"):\n                val_imgs = val_sample[\"image\"].cuda().float()[0]\n                val_preds_batch = []\n#                 for val_model in models:\n                val_x = train_model(model, val_imgs)\n                val_preds_batch.append(val_x.squeeze())\n                val_preds_batch = np.average(np.array(val_preds_batch), axis=0)\n                val_preds_batch = np.clip(val_preds_batch, 0.01, 0.99)\n                val_loss += loss_criterion(torch.tensor(val_preds_batch), val_sample['label'].squeeze()).item() * val_imgs.size(0)\n                val_preds.extend(val_preds_batch)\n                val_labels.extend(val_sample['label'].squeeze().cpu().numpy())\n        \n        val_loss /= len(val_loader.dataset)\n        val_accuracy = accuracy_score(np.array(val_labels), np.array(val_preds) >= 0.5)\n        print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f}\")\n\n        \n        if val_accuracy > best_metric:\n            best_metric = val_accuracy\n            best_metric_epoch = epoch + 1\n            best_metrics_epochs_and_time[0].append(best_metric)\n            best_metrics_epochs_and_time[1].append(best_metric_epoch)\n            best_metrics_epochs_and_time[2].append(time.time() - total_start)\n            torch.save(\n                model.state_dict(),\n                os.path.join(\"/kaggle/working/\", f\"deformableDETR_best_metric_model_{epoch+1}.pth\"),\n            )\n            print(\"saved new best metric model\")\n        else:\n            chk_file_name = \"deformableDETR_epoch_\" + str(epoch+1) + \"_model.pth\"    \n            torch.save(\n                model.state_dict(),\n                os.path.join(\"/kaggle/working/\", chk_file_name),\n            )","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:27:05.572082Z","iopub.execute_input":"2024-05-06T02:27:05.572397Z","iopub.status.idle":"2024-05-06T02:27:05.601444Z","shell.execute_reply.started":"2024-05-06T02:27:05.572368Z","shell.execute_reply":"2024-05-06T02:27:05.600329Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"code","source":"output = []\n\nstart_epoch = 0\nlatest_checkpoint_path = \"/kaggle/input/deformabledetr-checkpoint17/deformableDetr_epoch_95_model.pth\"\nif os.path.exists(latest_checkpoint_path):\n    checkpoint = torch.load(latest_checkpoint_path)\n    model.load_state_dict(checkpoint)\n    start_epoch = 95\n    print(\"checkpoint loaded\")\nelse:\n    print(\"No checkpoint. Starting from scratch\")\n# print(model)\ndevice=torch.device(\"cuda:0\")\nmodel = model.to(device)\ntrain_classification(model, cases, 0)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:27:05.602858Z","iopub.execute_input":"2024-05-06T02:27:05.603207Z","iopub.status.idle":"2024-05-06T02:27:07.321599Z","shell.execute_reply.started":"2024-05-06T02:27:05.603175Z","shell.execute_reply":"2024-05-06T02:27:07.320594Z"},"trusted":true},"outputs":[{"name":"stdout","text":"checkpoint loaded\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from typing import List\nimport torch\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\ndef predict_classification(models: List[nn.Module]):\n#     test_dataset = DatasetCrops(dataset_dir=test_dataset_dir, cases=cases)\n#     dataloader = DataLoader(\n#         test_dataset, batch_size=1, sampler=None, shuffle=False, num_workers=1, pin_memory=False\n#     )\n    \n    def predict_model(model, imgs):\n        preds = []\n        with torch.no_grad():\n            for i in range(len(imgs)):\n                with torch.cuda.amp.autocast():\n                    output = model(imgs[i:i + 1])\n                pred_slice = (output.float()).cpu().numpy().astype(np.float32)\n                with torch.cuda.amp.autocast():\n                    output = model(torch.flip(imgs[i:i + 1], dims=(-1,)))\n                pred_slice += (output.float()).cpu().numpy().astype(np.float32)\n                pred_slice /= 2\n                preds.append(pred_slice)\n        preds = np.max(np.array(preds), axis=0)\n        preds[np.isnan(preds)] = 0.01\n        return preds\n        \n    output_list = []  # Initialize the output list\n    all_labels = []\n    all_predictions = []\n    print(\".......................................................................\", len(test_loader))\n    for sample in tqdm(test_loader):\n        imgs = sample[\"image\"].cuda().float()[0]\n        cube_id = sample[\"cube_id\"][0]\n        labels = sample[\"label\"].cpu().numpy()\n        # Option 1: Remove outer list\n        labels = labels[0]\n        print(\"len(labels), labels\", len(labels), labels)\n        all_labels.extend(labels)\n        \n        with torch.no_grad():\n            preds = []\n            \n            preds.append(predict_model(model, imgs))  # Pass imgs to predict_model\n            preds = np.average(np.array(preds), axis=0)\n            preds = np.clip(preds, 0.01, 0.99)\n            print(preds)\n            all_predictions.extend(preds.squeeze())\n#             output_list.append([cube_id, preds])\n    \n    # Calculate metrics\n    \n    accuracy = accuracy_score(all_labels, (np.array(all_predictions) >= 0.5).astype(int))\n    precision = precision_score(all_labels, (np.array(all_predictions) >= 0.5).astype(int))\n    recall = recall_score(all_labels, (np.array(all_predictions) >= 0.5).astype(int))\n    f1 = f1_score(all_labels, (np.array(all_predictions) >= 0.5).astype(int))\n    print(\"accuracy, precision, recall, f1\", accuracy, precision, recall, f1)\n   # auc = roc_auc_score(all_labels, (np.array(all_predictions)))\n    \n    return output_list, accuracy, precision, recall, f1","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:27:07.323237Z","iopub.execute_input":"2024-05-06T02:27:07.323510Z","iopub.status.idle":"2024-05-06T02:27:07.341871Z","shell.execute_reply.started":"2024-05-06T02:27:07.323485Z","shell.execute_reply":"2024-05-06T02:27:07.340617Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"output = []\n# cases = os.listdir(train_dataset_dir)\npredict_classification(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:27:07.343290Z","iopub.execute_input":"2024-05-06T02:27:07.343655Z","iopub.status.idle":"2024-05-06T02:53:03.630295Z","shell.execute_reply.started":"2024-05-06T02:27:07.343624Z","shell.execute_reply":"2024-05-06T02:53:03.629348Z"},"trusted":true},"outputs":[{"name":"stdout","text":"....................................................................... 202\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/202 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"len(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 1/202 [00:06<21:41,  6.48s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57543945 0.47094727 0.34472656 0.55200195 0.5131836  0.36828613\n  0.5107422  0.5878906 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":"  1%|          | 2/202 [00:12<20:45,  6.23s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5732422  0.4753418  0.3466797  0.55444336 0.5136719  0.366333\n  0.5126953  0.59106445]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":"  1%|▏         | 3/202 [00:18<20:06,  6.06s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.595459   0.4852295  0.33044434 0.5422363  0.52490234 0.35717773\n  0.5241699  0.5722656 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":"  2%|▏         | 4/202 [00:24<19:28,  5.90s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5891113  0.46447754 0.34375    0.5461426  0.5251465  0.3675537\n  0.4859619  0.59350586]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":"  2%|▏         | 5/202 [00:30<19:54,  6.06s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5683594  0.48059082 0.34875488 0.54467773 0.5175781  0.36657715\n  0.5008545  0.59155273]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":"  3%|▎         | 6/202 [00:36<19:20,  5.92s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.58374023 0.4663086  0.3416748  0.55126953 0.52319336 0.36401367\n  0.49438477 0.5852051 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 1. 1.]\n","output_type":"stream"},{"name":"stderr","text":"  3%|▎         | 7/202 [00:42<19:51,  6.11s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57250977 0.46691895 0.34887695 0.54663086 0.50927734 0.36450195\n  0.5015869  0.59033203]]\nlen(labels), labels 8 [1. 0. 0. 0. 1. 1. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":"  4%|▍         | 8/202 [00:48<19:37,  6.07s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5727539  0.4724121  0.34375    0.54956055 0.517334   0.3692627\n  0.50439453 0.58984375]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":"  4%|▍         | 9/202 [00:56<20:58,  6.52s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.59033203 0.46325684 0.35375977 0.5371094  0.5065918  0.36291504\n  0.4921875  0.59033203]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":"  5%|▍         | 10/202 [01:02<20:59,  6.56s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57373047 0.4769287  0.34375    0.5510254  0.51049805 0.36950684\n  0.5048828  0.58813477]]\nlen(labels), labels 8 [1. 1. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":"  5%|▌         | 11/202 [01:08<20:17,  6.37s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5769043  0.47253418 0.3491211  0.5456543  0.5090332  0.36987305\n  0.4951172  0.58447266]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":"  6%|▌         | 12/202 [01:14<19:27,  6.15s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5908203  0.48620605 0.33166504 0.5270996  0.5344238  0.3527832\n  0.53344727 0.58569336]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":"  6%|▋         | 13/202 [01:21<19:55,  6.33s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57470703 0.46313477 0.34838867 0.5432129  0.5102539  0.36340332\n  0.51220703 0.58325195]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":"  7%|▋         | 14/202 [01:27<20:15,  6.46s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5732422  0.47937012 0.34851074 0.5493164  0.51220703 0.37036133\n  0.50268555 0.5878906 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":"  7%|▋         | 15/202 [01:37<22:57,  7.37s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5866699  0.4781494  0.33618164 0.5383301  0.5258789  0.36706543\n  0.49694824 0.5812988 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 16/202 [01:45<23:17,  7.51s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5722656  0.46875    0.34680176 0.5407715  0.50634766 0.36584473\n  0.50097656 0.5847168 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 17/202 [01:51<22:05,  7.17s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.576416   0.47399902 0.35253906 0.5432129  0.51293945 0.3671875\n  0.49597168 0.5905762 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":"  9%|▉         | 18/202 [01:57<21:16,  6.94s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5734863  0.4769287  0.3458252  0.546875   0.5109863  0.3642578\n  0.50683594 0.5908203 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":"  9%|▉         | 19/202 [02:06<22:39,  7.43s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57714844 0.4753418  0.3404541  0.54345703 0.5283203  0.3737793\n  0.49316406 0.5834961 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 10%|▉         | 20/202 [02:11<20:39,  6.81s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.59814453 0.47912598 0.32702637 0.52490234 0.53686523 0.35888672\n  0.52001953 0.5773926 ]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 21/202 [02:18<20:04,  6.65s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57836914 0.47607422 0.3466797  0.5551758  0.5131836  0.37487793\n  0.5114746  0.5891113 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 11%|█         | 22/202 [02:28<22:53,  7.63s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5739746  0.47314453 0.3416748  0.54907227 0.51171875 0.36743164\n  0.5012207  0.58691406]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 11%|█▏        | 23/202 [02:40<26:54,  9.02s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.578125   0.4645996  0.34509277 0.54345703 0.51342773 0.36364746\n  0.5008545  0.58496094]]\nlen(labels), labels 8 [1. 1. 1. 0. 1. 0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▏        | 24/202 [02:46<24:33,  8.28s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57250977 0.47253418 0.3479004  0.5456543  0.51123047 0.3671875\n  0.49694824 0.59228516]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 1. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▏        | 25/202 [02:53<22:44,  7.71s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5686035  0.4744873  0.3482666  0.5559082  0.50756836 0.3642578\n  0.50683594 0.592041  ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 13%|█▎        | 26/202 [03:02<24:23,  8.31s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5751953  0.4696045  0.34570312 0.5402832  0.5083008  0.36437988\n  0.4975586  0.58374023]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 13%|█▎        | 27/202 [03:09<22:49,  7.83s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5695801  0.47680664 0.34521484 0.54125977 0.52783203 0.37194824\n  0.4979248  0.58325195]]\nlen(labels), labels 8 [1. 1. 0. 1. 0. 1. 1. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 14%|█▍        | 28/202 [03:16<21:57,  7.57s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5812988  0.4663086  0.34802246 0.54345703 0.51416016 0.36608887\n  0.5041504  0.58447266]]\nlen(labels), labels 8 [1. 1. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 14%|█▍        | 29/202 [03:23<21:24,  7.43s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5913086  0.4675293  0.3375244  0.5378418  0.5126953  0.3659668\n  0.48706055 0.59375   ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▍        | 30/202 [03:30<20:28,  7.14s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5703125  0.477417   0.3395996  0.5439453  0.5175781  0.37023926\n  0.5083008  0.592041  ]]\nlen(labels), labels 8 [1. 0. 1. 1. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▌        | 31/202 [03:36<19:38,  6.89s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5678711  0.47729492 0.34570312 0.54541016 0.5131836  0.3642578\n  0.49829102 0.59472656]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▌        | 32/202 [03:40<17:25,  6.15s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.55126953 0.49719238 0.3955078  0.47680664 0.36730957 0.4197998\n  0.5546875  0.44006348]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 1. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▋        | 33/202 [03:47<17:54,  6.36s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57666016 0.47216797 0.34313965 0.5473633  0.513916   0.36462402\n  0.49621582 0.58935547]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 17%|█▋        | 34/202 [03:53<17:32,  6.26s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5776367  0.47070312 0.3486328  0.54663086 0.5095215  0.3671875\n  0.5007324  0.5854492 ]]\nlen(labels), labels 8 [1. 1. 0. 0. 0. 0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 17%|█▋        | 35/202 [04:00<18:07,  6.51s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5698242  0.47717285 0.34594727 0.5397949  0.5109863  0.35754395\n  0.52246094 0.58984375]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 36/202 [04:06<17:18,  6.26s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5698242  0.47387695 0.34570312 0.54248047 0.51831055 0.36584473\n  0.5078125  0.5817871 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 37/202 [04:12<16:55,  6.16s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5739746  0.48046875 0.3428955  0.54174805 0.51586914 0.36791992\n  0.50146484 0.5876465 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 19%|█▉        | 38/202 [04:18<16:22,  5.99s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.59814453 0.48413086 0.32922363 0.532959   0.5373535  0.35546875\n  0.5305176  0.58374023]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 1. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 19%|█▉        | 39/202 [04:24<16:25,  6.05s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5769043  0.4696045  0.3446045  0.5439453  0.51220703 0.36950684\n  0.4975586  0.58740234]]\nlen(labels), labels 8 [1. 0. 0. 0. 1. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 20%|█▉        | 40/202 [04:36<21:33,  7.99s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.58203125 0.46704102 0.34301758 0.5402832  0.513916   0.3701172\n  0.49206543 0.59155273]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 41/202 [04:43<20:08,  7.50s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.56884766 0.47229004 0.34960938 0.54296875 0.51708984 0.36767578\n  0.50561523 0.5883789 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 21%|██        | 42/202 [04:55<23:33,  8.84s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5876465  0.4847412  0.34204102 0.5319824  0.5209961  0.36169434\n  0.48571777 0.58251953]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 21%|██▏       | 43/202 [05:05<25:04,  9.46s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57373047 0.46276855 0.34448242 0.5439453  0.5114746  0.36901855\n  0.49414062 0.5839844 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 22%|██▏       | 44/202 [05:13<23:03,  8.76s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5756836  0.47216797 0.34631348 0.5493164  0.5151367  0.36987305\n  0.51049805 0.5888672 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 22%|██▏       | 45/202 [05:20<22:02,  8.43s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.56689453 0.47131348 0.34716797 0.54345703 0.51342773 0.36901855\n  0.4925537  0.5961914 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 23%|██▎       | 46/202 [05:26<20:04,  7.72s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.579834   0.47314453 0.34606934 0.5566406  0.51123047 0.36523438\n  0.51220703 0.59155273]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 23%|██▎       | 47/202 [05:39<23:34,  9.13s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5854492  0.4633789  0.34887695 0.53808594 0.51000977 0.35742188\n  0.4835205  0.5859375 ]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 24%|██▍       | 48/202 [05:45<21:36,  8.42s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5734863  0.47351074 0.34509277 0.55029297 0.5090332  0.36328125\n  0.5073242  0.58984375]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 24%|██▍       | 49/202 [05:52<20:07,  7.89s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5786133  0.4790039  0.34521484 0.54589844 0.5197754  0.36669922\n  0.491333   0.58984375]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▍       | 50/202 [06:02<21:07,  8.34s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5842285  0.4790039  0.3433838  0.54711914 0.5280762  0.368042\n  0.49499512 0.58984375]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▌       | 51/202 [06:15<24:56,  9.91s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.58203125 0.46777344 0.34936523 0.54003906 0.5083008  0.36376953\n  0.49279785 0.5810547 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 1. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 26%|██▌       | 52/202 [06:22<22:40,  9.07s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57250977 0.46936035 0.3482666  0.55322266 0.513916   0.36975098\n  0.5031738  0.5905762 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 26%|██▌       | 53/202 [06:29<21:06,  8.50s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.58032227 0.46398926 0.34680176 0.54785156 0.51464844 0.3701172\n  0.5007324  0.5817871 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 27%|██▋       | 54/202 [06:36<19:34,  7.93s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5708008  0.47473145 0.3466797  0.55444336 0.50878906 0.36523438\n  0.50219727 0.59375   ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 27%|██▋       | 55/202 [06:42<18:12,  7.43s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57177734 0.47509766 0.33898926 0.5444336  0.5126953  0.36047363\n  0.50805664 0.5908203 ]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 56/202 [06:48<16:55,  6.95s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5756836  0.46777344 0.34802246 0.54174805 0.50561523 0.36853027\n  0.4996338  0.5859375 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 57/202 [06:55<16:29,  6.83s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.576416   0.47729492 0.34631348 0.5541992  0.51049805 0.3626709\n  0.50683594 0.5864258 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 29%|██▊       | 58/202 [07:00<15:24,  6.42s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.58935547 0.48669434 0.33117676 0.51538086 0.5253906  0.3569336\n  0.5280762  0.5756836 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 29%|██▉       | 59/202 [07:12<19:04,  8.01s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5673828  0.45617676 0.3474121  0.54248047 0.4984131  0.37268066\n  0.48864746 0.57958984]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 30%|██▉       | 60/202 [07:18<17:24,  7.35s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.6032715  0.48095703 0.33190918 0.5307617  0.5354004  0.36975098\n  0.5180664  0.57788086]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 61/202 [07:27<18:51,  8.02s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5715332  0.4645996  0.34240723 0.54125977 0.50805664 0.3708496\n  0.50024414 0.5830078 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 31%|███       | 62/202 [07:35<18:50,  8.08s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5751953  0.46948242 0.3428955  0.5476074  0.51123047 0.36657715\n  0.50341797 0.58251953]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 31%|███       | 63/202 [07:43<18:08,  7.83s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57250977 0.47192383 0.3421631  0.5515137  0.5083008  0.36743164\n  0.51049805 0.5854492 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▏      | 64/202 [07:49<17:12,  7.48s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5695801  0.4625244  0.35302734 0.5461426  0.51000977 0.3623047\n  0.49475098 0.5944824 ]]\nlen(labels), labels 8 [1. 1. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▏      | 65/202 [07:56<16:39,  7.30s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5776367  0.47387695 0.34179688 0.545166   0.5185547  0.3647461\n  0.49206543 0.5834961 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 33%|███▎      | 66/202 [08:03<16:13,  7.16s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5703125  0.47436523 0.3453369  0.54785156 0.5065918  0.36657715\n  0.5008545  0.5905762 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 1. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 33%|███▎      | 67/202 [08:10<16:02,  7.13s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57128906 0.47192383 0.3511963  0.5444336  0.5090332  0.36791992\n  0.49230957 0.59375   ]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 34%|███▎      | 68/202 [08:17<15:30,  6.95s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57250977 0.47338867 0.34936523 0.5390625  0.5187988  0.3597412\n  0.50390625 0.58569336]]\nlen(labels), labels 8 [1. 0. 0. 0. 1. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 34%|███▍      | 69/202 [08:23<14:50,  6.70s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.58032227 0.47631836 0.3466797  0.54541016 0.51708984 0.36975098\n  0.50341797 0.5852051 ]]\nlen(labels), labels 8 [1. 1. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▍      | 70/202 [08:30<15:20,  6.97s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.576416   0.47351074 0.34313965 0.54663086 0.51586914 0.36157227\n  0.50878906 0.5854492 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 1. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▌      | 71/202 [08:40<17:05,  7.83s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5847168  0.4691162  0.34521484 0.53930664 0.51831055 0.35961914\n  0.49157715 0.5876465 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 72/202 [08:50<18:24,  8.50s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57299805 0.46826172 0.33813477 0.54003906 0.5073242  0.36450195\n  0.5024414  0.5827637 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 73/202 [08:59<18:19,  8.52s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5744629  0.46325684 0.34436035 0.53930664 0.5095215  0.36743164\n  0.49731445 0.58081055]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 37%|███▋      | 74/202 [09:04<16:13,  7.61s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57910156 0.47705078 0.34680176 0.54003906 0.5266113  0.368042\n  0.5019531  0.5883789 ]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 37%|███▋      | 75/202 [09:10<15:09,  7.16s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.58154297 0.4753418  0.34423828 0.5432129  0.51464844 0.36975098\n  0.4897461  0.5900879 ]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 76/202 [09:17<14:26,  6.88s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57470703 0.46875    0.34362793 0.54663086 0.5053711  0.3623047\n  0.5024414  0.5871582 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 77/202 [09:22<13:25,  6.45s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5827637  0.4814453  0.34301758 0.5432129  0.5161133  0.3630371\n  0.49975586 0.58984375]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 39%|███▊      | 78/202 [09:32<15:21,  7.43s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5786133  0.46289062 0.34228516 0.5419922  0.50683594 0.36767578\n  0.4925537  0.5866699 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 39%|███▉      | 79/202 [09:41<16:31,  8.06s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5788574  0.47021484 0.3453369  0.54467773 0.5229492  0.36743164\n  0.49731445 0.59399414]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 40%|███▉      | 80/202 [09:49<16:01,  7.88s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5734863  0.46350098 0.34423828 0.548584   0.51220703 0.3696289\n  0.5048828  0.5839844 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 81/202 [09:59<17:31,  8.69s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5722656  0.4625244  0.3416748  0.5366211  0.50927734 0.3671875\n  0.4897461  0.57958984]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 41%|████      | 82/202 [10:09<17:41,  8.84s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57421875 0.4683838  0.3461914  0.54248047 0.5131836  0.36413574\n  0.5073242  0.5830078 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 41%|████      | 83/202 [10:16<16:40,  8.41s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5722656  0.46813965 0.3461914  0.54541016 0.5126953  0.3671875\n  0.48608398 0.5810547 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 1. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 42%|████▏     | 84/202 [10:23<15:28,  7.87s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5703125  0.46740723 0.34753418 0.54663086 0.51220703 0.36657715\n  0.4984131  0.5891113 ]]\nlen(labels), labels 8 [1. 1. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 42%|████▏     | 85/202 [10:29<14:39,  7.52s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5744629  0.46533203 0.3474121  0.54711914 0.517334   0.36950684\n  0.50927734 0.59375   ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 43%|████▎     | 86/202 [10:38<15:02,  7.78s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5756836  0.4642334  0.33911133 0.5427246  0.5036621  0.3708496\n  0.49731445 0.58081055]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 43%|████▎     | 87/202 [10:47<15:57,  8.32s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57470703 0.46679688 0.34436035 0.5397949  0.5061035  0.36828613\n  0.50097656 0.58032227]]\nlen(labels), labels 8 [1. 1. 1. 0. 0. 0. 1. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 44%|████▎     | 88/202 [10:54<14:48,  7.79s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.58203125 0.4802246  0.34716797 0.54785156 0.5151367  0.37438965\n  0.5090332  0.58740234]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 1. 1. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 44%|████▍     | 89/202 [11:01<14:20,  7.62s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5749512  0.46923828 0.3470459  0.5449219  0.5126953  0.3696289\n  0.5029297  0.58374023]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 45%|████▍     | 90/202 [11:07<13:16,  7.11s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5749512  0.4703369  0.3428955  0.5456543  0.5085449  0.36242676\n  0.49902344 0.5876465 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 45%|████▌     | 91/202 [11:13<12:44,  6.88s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5673828  0.4736328  0.350708   0.54833984 0.5107422  0.36657715\n  0.50634766 0.58935547]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 46%|████▌     | 92/202 [11:19<12:07,  6.61s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57543945 0.48254395 0.34924316 0.5415039  0.5168457  0.3581543\n  0.5114746  0.58032227]]\nlen(labels), labels 8 [1. 1. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 46%|████▌     | 93/202 [11:25<11:38,  6.41s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5749512  0.46801758 0.3461914  0.5427246  0.5168457  0.36083984\n  0.5001221  0.58813477]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 47%|████▋     | 94/202 [11:38<14:49,  8.24s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57592773 0.47180176 0.33911133 0.54296875 0.51293945 0.36132812\n  0.49658203 0.5949707 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 1. 1. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 47%|████▋     | 95/202 [11:46<14:51,  8.33s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.576416   0.4790039  0.33911133 0.54125977 0.52197266 0.37268066\n  0.49401855 0.58935547]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 96/202 [11:52<13:11,  7.47s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.59277344 0.48657227 0.32824707 0.5280762  0.5332031  0.35437012\n  0.54345703 0.58862305]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 97/202 [12:03<15:00,  8.58s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57128906 0.47265625 0.3503418  0.5422363  0.51464844 0.36206055\n  0.48791504 0.58862305]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 49%|████▊     | 98/202 [12:12<15:22,  8.87s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5854492  0.47314453 0.34729004 0.5385742  0.5241699  0.36572266\n  0.48510742 0.5827637 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 49%|████▉     | 99/202 [12:24<16:25,  9.57s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5744629  0.47717285 0.34570312 0.5415039  0.5119629  0.35888672\n  0.49243164 0.58984375]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 50%|████▉     | 100/202 [12:31<15:01,  8.84s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57470703 0.47143555 0.34069824 0.54541016 0.51049805 0.36608887\n  0.5029297  0.58032227]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 101/202 [12:38<13:55,  8.27s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5690918  0.47192383 0.3479004  0.5500488  0.515625   0.36669922\n  0.5058594  0.58325195]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 102/202 [12:44<12:52,  7.72s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5727539  0.4654541  0.3453369  0.5493164  0.5109863  0.36364746\n  0.5029297  0.5871582 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 51%|█████     | 103/202 [12:54<13:42,  8.30s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.56884766 0.46472168 0.34484863 0.5407715  0.5073242  0.36901855\n  0.50219727 0.5842285 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 51%|█████▏    | 104/202 [13:05<14:45,  9.03s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.58374023 0.47106934 0.34179688 0.5422363  0.5266113  0.36108398\n  0.491333   0.5891113 ]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 52%|█████▏    | 105/202 [13:11<13:16,  8.21s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.579834   0.46813965 0.34313965 0.5517578  0.5163574  0.36816406\n  0.4991455  0.58984375]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 1. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 52%|█████▏    | 106/202 [13:17<12:02,  7.52s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5793457  0.47473145 0.34814453 0.54907227 0.5161133  0.36779785\n  0.50756836 0.5942383 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 53%|█████▎    | 107/202 [13:27<13:15,  8.37s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5769043  0.4638672  0.3441162  0.5383301  0.50805664 0.3656006\n  0.49829102 0.5847168 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 53%|█████▎    | 108/202 [13:41<15:32,  9.92s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5769043  0.47155762 0.3466797  0.54248047 0.50878906 0.36206055\n  0.4967041  0.5925293 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 54%|█████▍    | 109/202 [13:48<14:16,  9.21s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.58618164 0.46398926 0.34936523 0.54248047 0.51586914 0.36572266\n  0.49316406 0.58691406]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 54%|█████▍    | 110/202 [13:55<13:01,  8.50s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57250977 0.46594238 0.34692383 0.54956055 0.5144043  0.36914062\n  0.5058594  0.58740234]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▍    | 111/202 [14:01<11:30,  7.59s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.59521484 0.47875977 0.32983398 0.53564453 0.53271484 0.352417\n  0.50634766 0.5761719 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 1. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▌    | 112/202 [14:07<10:51,  7.24s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5703125  0.46484375 0.34765625 0.54467773 0.50878906 0.36816406\n  0.5109863  0.5913086 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 1. 1. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 56%|█████▌    | 113/202 [14:13<10:06,  6.81s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57592773 0.47338867 0.34655762 0.5505371  0.5109863  0.36523438\n  0.51464844 0.58984375]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 56%|█████▋    | 114/202 [14:26<12:45,  8.69s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57543945 0.46740723 0.3458252  0.5432129  0.5078125  0.36340332\n  0.49365234 0.59350586]]\nlen(labels), labels 8 [1. 0. 0. 1. 1. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 57%|█████▋    | 115/202 [14:32<11:19,  7.81s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57788086 0.4645996  0.34753418 0.5383301  0.5163574  0.36218262\n  0.5041504  0.5895996 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 57%|█████▋    | 116/202 [14:37<10:11,  7.12s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.60668945 0.4827881  0.3272705  0.5324707  0.52856445 0.36206055\n  0.5222168  0.5708008 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 58%|█████▊    | 117/202 [14:43<09:35,  6.77s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5710449  0.47277832 0.35168457 0.5385742  0.5197754  0.36608887\n  0.51123047 0.5876465 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 58%|█████▊    | 118/202 [14:50<09:22,  6.69s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57470703 0.47766113 0.34570312 0.5476074  0.5175781  0.3626709\n  0.49719238 0.59033203]]\nlen(labels), labels 8 [1. 0. 0. 0. 1. 0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 59%|█████▉    | 119/202 [14:55<08:48,  6.37s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5805664  0.4663086  0.34875488 0.5510254  0.52563477 0.3618164\n  0.48608398 0.5900879 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 1. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 59%|█████▉    | 120/202 [15:02<08:46,  6.42s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5722656  0.4798584  0.34875488 0.5390625  0.51123047 0.3647461\n  0.5070801  0.58618164]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 60%|█████▉    | 121/202 [15:08<08:36,  6.37s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5715332  0.47375488 0.34643555 0.5515137  0.5131836  0.3746338\n  0.50561523 0.5891113 ]]\nlen(labels), labels 8 [1. 1. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 122/202 [15:18<09:48,  7.35s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5761719  0.47583008 0.34375    0.5395508  0.52001953 0.36657715\n  0.49682617 0.5788574 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 1. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 61%|██████    | 123/202 [15:24<09:18,  7.07s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.56811523 0.47180176 0.3441162  0.5461426  0.50805664 0.36584473\n  0.49609375 0.5917969 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 61%|██████▏   | 124/202 [15:32<09:21,  7.20s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5751953  0.47521973 0.34057617 0.5527344  0.513916   0.36547852\n  0.51416016 0.5974121 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 62%|██████▏   | 125/202 [15:38<09:03,  7.06s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5739746  0.4741211  0.33984375 0.54541016 0.5180664  0.37316895\n  0.49279785 0.59155273]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 62%|██████▏   | 126/202 [15:45<08:57,  7.07s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5727539  0.47387695 0.35144043 0.55371094 0.513916   0.366333\n  0.5065918  0.59472656]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 63%|██████▎   | 127/202 [15:57<10:28,  8.38s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5769043  0.4671631  0.3428955  0.5354004  0.5085449  0.37097168\n  0.49316406 0.58374023]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 1. 1. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 63%|██████▎   | 128/202 [16:09<11:36,  9.41s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5854492  0.48120117 0.34240723 0.5358887  0.5180664  0.36669922\n  0.4954834  0.5864258 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 64%|██████▍   | 129/202 [16:19<11:43,  9.63s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5866699  0.4654541  0.3470459  0.5427246  0.5058594  0.36743164\n  0.4996338  0.58691406]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 64%|██████▍   | 130/202 [16:31<12:38, 10.53s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.58154297 0.4658203  0.34716797 0.5371094  0.5070801  0.36779785\n  0.4967041  0.5883789 ]]\nlen(labels), labels 8 [1. 1. 0. 0. 0. 1. 1. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 65%|██████▍   | 131/202 [16:39<11:22,  9.61s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57421875 0.46875    0.34570312 0.5456543  0.51416016 0.36730957\n  0.5090332  0.5827637 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 65%|██████▌   | 132/202 [16:45<10:04,  8.64s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.56933594 0.47058105 0.34680176 0.5415039  0.5161133  0.36218262\n  0.50561523 0.58569336]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 66%|██████▌   | 133/202 [16:55<10:24,  9.05s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5800781  0.47558594 0.34570312 0.54125977 0.5263672  0.36669922\n  0.4868164  0.5883789 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 66%|██████▋   | 134/202 [17:01<09:09,  8.08s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57177734 0.47131348 0.34399414 0.55200195 0.51171875 0.37072754\n  0.50878906 0.58862305]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 67%|██████▋   | 135/202 [17:07<08:08,  7.28s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5769043  0.46594238 0.33728027 0.5427246  0.5229492  0.36132812\n  0.49121094 0.5834961 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 67%|██████▋   | 136/202 [17:12<07:27,  6.77s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5800781  0.47338867 0.3413086  0.53881836 0.52734375 0.3659668\n  0.49816895 0.5817871 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 68%|██████▊   | 137/202 [17:21<07:56,  7.34s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5788574  0.4798584  0.34118652 0.54248047 0.52124023 0.36999512\n  0.5004883  0.59399414]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 68%|██████▊   | 138/202 [17:26<07:16,  6.82s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.58862305 0.47375488 0.34472656 0.53808594 0.5263672  0.36047363\n  0.4892578  0.5847168 ]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 69%|██████▉   | 139/202 [17:34<07:21,  7.00s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5773926  0.46313477 0.34448242 0.5410156  0.52001953 0.36120605\n  0.5090332  0.58691406]]\nlen(labels), labels 8 [1. 1. 1. 0. 0. 1. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 69%|██████▉   | 140/202 [17:41<07:09,  6.93s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5810547  0.47729492 0.34436035 0.5456543  0.5175781  0.36657715\n  0.48840332 0.5859375 ]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 70%|██████▉   | 141/202 [17:47<06:45,  6.65s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.59106445 0.48339844 0.33154297 0.5493164  0.5263672  0.3585205\n  0.52124023 0.5930176 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 142/202 [17:54<06:55,  6.92s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5695801  0.47314453 0.34094238 0.54003906 0.5078125  0.3630371\n  0.513916   0.58813477]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 71%|███████   | 143/202 [18:08<09:00,  9.16s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.58935547 0.4696045  0.34960938 0.54296875 0.5097656  0.3642578\n  0.4876709  0.59277344]]\nlen(labels), labels 8 [1. 0. 0. 0. 1. 1. 1. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 71%|███████▏  | 144/202 [18:20<09:36,  9.94s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.58251953 0.47875977 0.34277344 0.5402832  0.5205078  0.36437988\n  0.48742676 0.58740234]]\nlen(labels), labels 8 [1. 1. 0. 1. 0. 1. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▏  | 145/202 [18:27<08:27,  8.91s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5720215  0.46801758 0.3527832  0.54248047 0.50756836 0.35986328\n  0.49291992 0.58691406]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▏  | 146/202 [18:37<08:46,  9.40s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5810547  0.47277832 0.34020996 0.5361328  0.5214844  0.36669922\n  0.4918213  0.5871582 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 1. 1. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 73%|███████▎  | 147/202 [18:50<09:23, 10.25s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5734863  0.4769287  0.34143066 0.53930664 0.5266113  0.36657715\n  0.49645996 0.58862305]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 1. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 73%|███████▎  | 148/202 [18:59<08:57,  9.96s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5957031  0.47558594 0.34020996 0.53686523 0.51464844 0.36279297\n  0.48950195 0.5966797 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 74%|███████▍  | 149/202 [19:05<07:51,  8.89s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.56591797 0.48132324 0.34448242 0.5407715  0.51342773 0.36401367\n  0.5107422  0.5949707 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 74%|███████▍  | 150/202 [19:13<07:27,  8.60s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57470703 0.47729492 0.34387207 0.5437012  0.51660156 0.3692627\n  0.49597168 0.58618164]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 75%|███████▍  | 151/202 [19:20<06:54,  8.14s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5673828  0.47875977 0.3470459  0.54125977 0.51538086 0.36499023\n  0.5124512  0.5925293 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 75%|███████▌  | 152/202 [19:27<06:30,  7.81s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57788086 0.46435547 0.34301758 0.5415039  0.51123047 0.37316895\n  0.5003662  0.58154297]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 76%|███████▌  | 153/202 [19:33<05:46,  7.06s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.60253906 0.4802246  0.3277588  0.5332031  0.53515625 0.36083984\n  0.5253906  0.5749512 ]]\nlen(labels), labels 8 [1. 1. 0. 0. 0. 1. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 76%|███████▌  | 154/202 [19:47<07:23,  9.24s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5769043  0.47436523 0.3425293  0.5397949  0.51220703 0.3602295\n  0.49291992 0.5913086 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 77%|███████▋  | 155/202 [19:52<06:19,  8.07s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.59814453 0.48010254 0.33068848 0.53637695 0.52563477 0.35595703\n  0.5263672  0.5842285 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 77%|███████▋  | 156/202 [19:57<05:31,  7.20s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57299805 0.47314453 0.33935547 0.54248047 0.52783203 0.36035156\n  0.5024414  0.58447266]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 78%|███████▊  | 157/202 [20:09<06:22,  8.51s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57666016 0.47717285 0.33972168 0.53466797 0.51782227 0.36669922\n  0.4918213  0.5883789 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 78%|███████▊  | 158/202 [20:19<06:29,  8.86s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.579834   0.47680664 0.33996582 0.5397949  0.52368164 0.36791992\n  0.49719238 0.5905762 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 79%|███████▊  | 159/202 [20:27<06:10,  8.62s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5756836  0.4638672  0.34716797 0.54125977 0.50683594 0.3659668\n  0.5007324  0.58447266]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 79%|███████▉  | 160/202 [20:33<05:36,  8.02s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5727539  0.46557617 0.34448242 0.54663086 0.5148926  0.37268066\n  0.5058594  0.5854492 ]]\nlen(labels), labels 8 [1. 0. 1. 1. 0. 0. 1. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 80%|███████▉  | 161/202 [20:39<04:59,  7.31s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.60375977 0.4781494  0.3428955  0.5292969  0.53149414 0.35632324\n  0.51953125 0.58325195]]\nlen(labels), labels 8 [1. 0. 0. 0. 1. 0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 162/202 [20:45<04:41,  7.05s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.58618164 0.47351074 0.3433838  0.5397949  0.5317383  0.36169434\n  0.49487305 0.58569336]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 81%|████████  | 163/202 [20:51<04:16,  6.58s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5817871  0.47680664 0.3428955  0.5407715  0.52246094 0.3614502\n  0.49694824 0.5842285 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 81%|████████  | 164/202 [21:01<04:54,  7.76s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57373047 0.4724121  0.33728027 0.53881836 0.5197754  0.3651123\n  0.49279785 0.58691406]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 82%|████████▏ | 165/202 [21:12<05:17,  8.59s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5812988  0.4703369  0.34265137 0.54248047 0.5187988  0.368042\n  0.48706055 0.5900879 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 1. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 82%|████████▏ | 166/202 [21:19<04:50,  8.07s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.579834   0.46691895 0.3466797  0.54345703 0.5126953  0.3684082\n  0.5078125  0.5878906 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 83%|████████▎ | 167/202 [21:28<04:51,  8.34s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5864258  0.47473145 0.34362793 0.5410156  0.52001953 0.36657715\n  0.48498535 0.5866699 ]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 83%|████████▎ | 168/202 [21:34<04:23,  7.76s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5793457  0.46972656 0.34924316 0.5473633  0.5151367  0.35717773\n  0.49731445 0.5913086 ]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 84%|████████▎ | 169/202 [21:46<04:57,  9.01s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5895996  0.46813965 0.3470459  0.5419922  0.5061035  0.36291504\n  0.49682617 0.5949707 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 84%|████████▍ | 170/202 [21:55<04:52,  9.13s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.578125   0.46374512 0.3453369  0.5439453  0.5061035  0.3687744\n  0.49743652 0.5842285 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 85%|████████▍ | 171/202 [22:03<04:32,  8.77s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.58081055 0.47399902 0.3375244  0.5385742  0.52124023 0.3675537\n  0.49023438 0.58813477]]\nlen(labels), labels 8 [1. 1. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 85%|████████▌ | 172/202 [22:10<04:01,  8.05s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5739746  0.48046875 0.34240723 0.54467773 0.52441406 0.36499023\n  0.49560547 0.58862305]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 86%|████████▌ | 173/202 [22:15<03:30,  7.27s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5749512  0.47473145 0.34521484 0.5510254  0.5300293  0.36169434\n  0.48754883 0.5859375 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 86%|████████▌ | 174/202 [22:20<03:06,  6.67s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.6008301  0.47973633 0.3380127  0.52441406 0.5324707  0.35424805\n  0.52563477 0.5842285 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 1. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 87%|████████▋ | 175/202 [22:28<03:05,  6.88s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.58081055 0.46520996 0.352417   0.54345703 0.5013428  0.3557129\n  0.48815918 0.5900879 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 87%|████████▋ | 176/202 [22:34<02:56,  6.77s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5751953  0.46691895 0.3474121  0.5488281  0.51416016 0.37072754\n  0.5046387  0.5842285 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 88%|████████▊ | 177/202 [22:48<03:42,  8.90s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.579834   0.47668457 0.34472656 0.5371094  0.51660156 0.36462402\n  0.48608398 0.5949707 ]]\nlen(labels), labels 8 [1. 0. 1. 1. 1. 1. 1. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 88%|████████▊ | 178/202 [22:55<03:17,  8.23s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5834961  0.4741211  0.34802246 0.548584   0.513916   0.36730957\n  0.51049805 0.5895996 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 89%|████████▊ | 179/202 [23:01<02:53,  7.54s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.58325195 0.47705078 0.34838867 0.5358887  0.5246582  0.3618164\n  0.49230957 0.5839844 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":" 89%|████████▉ | 180/202 [23:07<02:37,  7.14s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.59375    0.48571777 0.32336426 0.5410156  0.5358887  0.35620117\n  0.5239258  0.5800781 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 90%|████████▉ | 181/202 [23:13<02:21,  6.73s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5817871  0.46887207 0.3421631  0.54785156 0.5229492  0.3687744\n  0.4946289  0.5859375 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 182/202 [23:18<02:05,  6.29s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5839844  0.47595215 0.3425293  0.5378418  0.52441406 0.36572266\n  0.49365234 0.58740234]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 91%|█████████ | 183/202 [23:24<01:59,  6.30s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57373047 0.47277832 0.34155273 0.5422363  0.5131836  0.36938477\n  0.5065918  0.5878906 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 1. 1. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 91%|█████████ | 184/202 [23:31<01:55,  6.40s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57470703 0.47229004 0.3479004  0.5439453  0.51342773 0.36645508\n  0.51000977 0.5834961 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 92%|█████████▏| 185/202 [23:37<01:48,  6.40s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5722656  0.46618652 0.34753418 0.5461426  0.5209961  0.3671875\n  0.5046387  0.58935547]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 92%|█████████▏| 186/202 [23:44<01:45,  6.58s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5776367  0.4708252  0.34228516 0.5427246  0.51123047 0.36669922\n  0.51708984 0.5871582 ]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 93%|█████████▎| 187/202 [23:50<01:32,  6.19s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5891113  0.47351074 0.34777832 0.54956055 0.5119629  0.3651123\n  0.5024414  0.58740234]]\nlen(labels), labels 8 [1. 0. 1. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 93%|█████████▎| 188/202 [24:02<01:51,  7.95s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5793457  0.47253418 0.350708   0.5432129  0.5107422  0.36975098\n  0.48413086 0.59155273]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 94%|█████████▎| 189/202 [24:11<01:46,  8.20s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5895996  0.4790039  0.3453369  0.5378418  0.5214844  0.366333\n  0.49279785 0.5854492 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 1. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 94%|█████████▍| 190/202 [24:18<01:33,  7.83s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5690918  0.4798584  0.3404541  0.54296875 0.51342773 0.36621094\n  0.50024414 0.58984375]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 95%|█████████▍| 191/202 [24:25<01:25,  7.73s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5727539  0.4609375  0.34838867 0.54785156 0.51123047 0.36669922\n  0.5053711  0.5859375 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 95%|█████████▌| 192/202 [24:31<01:11,  7.12s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5761719  0.47131348 0.34423828 0.54785156 0.50927734 0.36669922\n  0.50097656 0.5871582 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 96%|█████████▌| 193/202 [24:40<01:10,  7.80s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57373047 0.46411133 0.3433838  0.5415039  0.5070801  0.36889648\n  0.49572754 0.58203125]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 96%|█████████▌| 194/202 [24:50<01:07,  8.49s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.58203125 0.47351074 0.3375244  0.5432129  0.5214844  0.36462402\n  0.48864746 0.5888672 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 97%|█████████▋| 195/202 [24:57<00:56,  8.04s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5637207  0.4741211  0.34326172 0.5437012  0.5124512  0.36950684\n  0.4918213  0.592041  ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 97%|█████████▋| 196/202 [25:09<00:54,  9.16s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5722656  0.47045898 0.3441162  0.5449219  0.51171875 0.3635254\n  0.4909668  0.58813477]]\nlen(labels), labels 8 [1. 0. 0. 0. 1. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 98%|█████████▊| 197/202 [25:17<00:43,  8.69s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5749512  0.46154785 0.34375    0.5463867  0.50756836 0.37182617\n  0.51000977 0.5859375 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 98%|█████████▊| 198/202 [25:27<00:37,  9.31s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57666016 0.46411133 0.34594727 0.5371094  0.51171875 0.36486816\n  0.48815918 0.58251953]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▊| 199/202 [25:34<00:25,  8.57s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.57592773 0.47094727 0.34594727 0.5517578  0.5239258  0.3713379\n  0.50878906 0.5847168 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 200/202 [25:40<00:15,  7.85s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5776367  0.4798584  0.34313965 0.5498047  0.5180664  0.37426758\n  0.49682617 0.5883789 ]]\nlen(labels), labels 8 [1. 0. 0. 0. 0. 0. 1. 1.]\n","output_type":"stream"},{"name":"stderr","text":"100%|█████████▉| 201/202 [25:46<00:07,  7.12s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.5800781  0.47192383 0.34753418 0.54345703 0.5209961  0.3684082\n  0.49499512 0.5864258 ]]\nlen(labels), labels 8 [0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 202/202 [25:56<00:00,  7.70s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.576416   0.47253418 0.34851074 0.5456543  0.52001953 0.36706543\n  0.48779297 0.5878906 ]]\naccuracy, precision, recall, f1 0.4839108910891089 0.17089305402425578 0.6540084388185654 0.270979020979021\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"([],\n 0.4839108910891089,\n 0.17089305402425578,\n 0.6540084388185654,\n 0.270979020979021)"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:53:03.631858Z","iopub.execute_input":"2024-05-06T02:53:03.632248Z","iopub.status.idle":"2024-05-06T02:53:03.862015Z","shell.execute_reply.started":"2024-05-06T02:53:03.632209Z","shell.execute_reply":"2024-05-06T02:53:03.860619Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/116484730.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/working/seg_preds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;31m# lstat()/open()/fstat() trick.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/seg_preds'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/seg_preds'","output_type":"error"}],"execution_count":27}]}